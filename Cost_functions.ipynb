{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{(\\ll)}\n",
       "%\n",
       "\\newcommand{\\tp}{\\mathbf{{(t)}}}\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro `_latex_std_` created. To execute, type its name (without quotes).\n",
      "=== Macro contents: ===\n",
      "get_ipython().run_line_magic('run', 'Latex_macros.ipynb')\n",
      " "
     ]
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "$\n",
    "\\newcommand{\\pdata}{\\prob_\\text{data}}\n",
    "\\newcommand{\\pmodel}{\\prob_\\text{model}}\n",
    "\\newcommand{\\likeli}{\\mathbb{L}}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# My standard magic !  You will see this in almost all my notebooks.\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Reload all modules imported with %aimport\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cost functions: Classical Machine Learning\n",
    "\n",
    "We have thus far presented cost functions for Regression and Classification\n",
    "- Mean Squared Error (Regression)\n",
    "- Cross Entropy (Classification)\n",
    "\n",
    "with little more than \"intuitive justification.\n",
    "\n",
    "We now explain them more mathematically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In Classical Machine Learning, a technique called *Maximum Likelihood* is the basis\n",
    "for many algorithms.\n",
    "\n",
    "We explain this idea and show how the cost functions encountered thus far can be explained in terms\n",
    "of likelihood maximization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised prediction as likelihood\n",
    "\n",
    "In the Classification task, we are predicting a distribution of values\n",
    "rather than a single value.\n",
    "\n",
    "The Regression task, at first glance, appears to predict a single value rather than a distribution of values.\n",
    "\n",
    "There is an interpretaton of the Regression task that views it as also predicting a distribution of values.\n",
    "\n",
    "This will be very useful in explaining where the Cost function comes from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It's possible to have two training examples $i, i'$ with identical features but different targets\n",
    "- $\\x^\\ip = \\x^{(i')}$ but\n",
    "- $\\y^\\ip \\ne \\y^{(i')}$\n",
    "\n",
    "This means that the estimator is not a function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A simple explanation is one of measurement error\n",
    "- Imprecision in measuring the targets $\\y^\\ip, \\y^{(i')}$\n",
    "    - There is a \"true\" (in terms of a function mapping) $\\tilde{\\y}^\\ip$ such that\n",
    "        - $\\y^\\ip   =  \\tilde{\\y}^\\ip + \\epsilon^\\ip$\n",
    "        - $\\y^{(i')} =  \\tilde{\\y}^\\ip + \\epsilon^{(i')}$\n",
    "    - i.e., the two targets are really the same $\\tilde{\\y}^\\ip$, but have been mis-measured\n",
    "- Imprecision in measuring features $\\x^\\ip, \\x^{(i')}$\n",
    "    - the \"true\" feature values are *different* $\\tilde{\\x}^\\ip \\ne \\tilde{\\x}^{(i')}$ but mis-measured as equal  \n",
    "        - $\\x^\\ip      =  \\tilde{\\x}^\\ip + \\epsilon^\\ip$\n",
    "        - $\\x^{(i')}   =  \\tilde{\\x}^{(i')} + \\epsilon^{(i')}$\n",
    "\n",
    "So rather than our model (estimator) predicting a single value, it predicts a distribution of values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Can frame the Supervised Learning task as being creating a model\n",
    "to predict \n",
    "$$\n",
    "\\pr{\\y^\\ip | x^\\ip}\n",
    "$$\n",
    "the *conditional probability* of $\\y^\\ip$ given input $\\x^\\ip$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Log likelihood\n",
    "[Deep Learning Book 5.5](https://www.deeplearningbook.org/contents/ml.html)\n",
    "\n",
    "The training data $\\{ x^\\ip, \\y^\\ip | i=1, \\dots, m \\}$\n",
    "is a sample from an unknown joint distribution $\\pdata(\\x, \\y)$.\n",
    "\n",
    "So the training data is an empirical distribution of some true but unknown  underlying $\\pdata(\\x, \\y)$\n",
    "\n",
    "A model (parameterized by $\\Theta$) creates an *approximation* $\\pmodel(\\x, \\y;\\Theta)$ of $\\pdata(\\x, \\y)$.\n",
    "\n",
    "Note that $\\pdata(\\x, \\y)$ is not parameterized by $\\Theta$.\n",
    "\n",
    "We can motivate the choice of $\\Theta$ by the principle of Likelihood Maximization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Given the training set, and the true joint distribution $\\pdata(\\x, \\y)$,\n",
    "we can compute the likelihood (i.e, probability) of drawing the $m$ samples in the training set as\n",
    "\n",
    "$$\n",
    "\\likeli_{\\text{data}} = \\prod_{i=1}^m { \\pdata(\\x^\\ip, \\y^\\ip) }\n",
    "$$\n",
    "\n",
    "Similarly, we can compute the same likelihood, using the probabilities from the model\n",
    "\n",
    "$$\n",
    "\\likeli_{\\text{model}} = \\prod_{i=1}^m { \\pmodel(\\x^\\ip, \\y^\\ip; \\Theta) }\n",
    "$$\n",
    "\n",
    "We can turn this product into a sum by taking the log of both sides\n",
    "\n",
    "$$\n",
    "\\log(\\likeli_{\\text{model}}) = \\sum_{i=1}^m { \\log(\\pmodel(\\x^\\ip, \\y^\\ip; \\Theta)) }\n",
    "$$\n",
    "\n",
    "This is called the Log Likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How do we choose $\\Theta$ ?\n",
    "\n",
    "Let us choose $\\Theta$ such that the choice\n",
    "*maximizes* the likelihood of seeing the training set.\n",
    "\n",
    "$$\n",
    "\\hat{\\Theta} = \\argmax{\\Theta}{\\sum_{i=1}^m { \\log(\\pmodel(\\x^\\ip, \\y^\\ip; \\Theta)) } }\n",
    "$$\n",
    "\n",
    "That is, the choice of $\\Theta$ that results in a model which best approximates the empirical (training) data.\n",
    "\n",
    "Finding the best $\\Theta$ means maximizing the log likelihood of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## KL divergence\n",
    "\n",
    "We can now motivated the KL divergence:\n",
    "- the difference between the log likelihood\n",
    "of the empirical and model distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Bayes Theorem relates joint and conditional probabilities\n",
    "\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\pr{\\y | \\x } & = & \\frac{\\pr{\\x,\\y}} {\\pr{\\x}} \\\\\n",
    "\\pr{\\x,\\y} & = & \\pr{\\y | \\x} \\; \\pr{\\x}  & \\text{re-arrrange the terms} \\\\\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "So we can re-write\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\log(\\likeli_{\\text{model}}) & = & \\sum_{i=1}^m { \\log(\\pmodel(\\x^\\ip, \\y^\\ip; \\Theta)) } \\\\\n",
    "                             & = & \\sum_{i=1}^m { \\log(\\pmodel(\\y^\\ip | \\x^\\ip ; \\Theta)) \\; \\pr{\\x^\\ip} } \\\\\n",
    "                             & = & \\E_{\\x \\sim \\pdata}  {\\log(\\pmodel(\\y^\\ip | \\x^\\ip ; \\Theta))} \n",
    "\\end{array}\n",
    "$$\n",
    "and similarly for $\\log(\\likeli_{\\text{data}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The difference between the log likelihoods of the two distributions \n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\log(\\likeli_{\\text{data}}) - \\log(\\likeli_{\\text{model}}) & = &\n",
    "    \\E_{\\x \\sim \\pdata} { \\left( \\log(\\pdata(\\y | \\x))  \\right) }  - \\E_{\\x \\sim \\pdata} { \\left( \\log(\\pmodel(\\y | \\x; \\Theta)) \\right)} \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "You hopefully recognize the difference as being equal\n",
    "to the definition of KL Divergence.\n",
    "\n",
    "Thus, the KL divergence is explained as the difference between the log likelihoods\n",
    "of the empirical and model distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cost functions for Classical Machine Learning\n",
    "We now show that our choice of cost functions\n",
    "- MSE for Regression\n",
    "- Binary Cross Entropy for (binary) Classification\n",
    "\n",
    "can be justified in terms of\n",
    "maximization of the log likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Log likelihood of Binary classifiction\n",
    "\n",
    "For binary classification (where $\\hat{y}^\\ip \\in \\{0,1\\}$)\n",
    "we compute a score as a linear function of $\\x$\n",
    "$$\n",
    "s(\\x^\\ip) = \\Theta^T \\cdot \\x^\\ip\n",
    "$$\n",
    "\n",
    "and we convert the linear score into a probability via the logistic function\n",
    "\n",
    "$$ \\hat{p}^\\ip  = \\sigma(s(\\hat\\x^\\ip))$$\n",
    "\n",
    "A Positive prediction (i.e., prediction of value $1$) is the conditional probability\n",
    "$$\n",
    "p(\\hat{y}^\\ip  = 1 \\, |\\, \\x^\\ip) = \\hat{p}^\\ip\n",
    "$$\n",
    "And a Negative prediction (i.e., prediction of value $0$) is\n",
    "$$\n",
    "p(\\hat{y}^\\ip  = 0 \\,|\\, \\x^\\ip) = 1 - \\hat{p}^\\ip\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can combine the equations for the two cases into the single equation\n",
    "$$\n",
    "p(\\hat{y}^\\ip | \\x^\\ip) = p(\\hat{y}^\\ip  = 1  | \\x^\\ip)^{\\y^\\ip} * p(\\hat{y}^\\ip  = 0 | \\x^\\ip)^{(1- \\y^\\ip)}\n",
    "$$\n",
    "\n",
    "(because $\\y \\in \\{0,1\\}$, one term in the product  always has exponent $0$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Again, the likelihood is the product (over $i$) of these terms and the log likelihood is\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\mathcal{l} & = & \\sum_{i=1}^m { \\y^\\ip \\log(p(\\hat{y}^\\ip  = 1  | \\x^\\ip))  + (1-\\y^\\ip )\\log(p(\\hat{y}^\\ip  = 0  | \\x^\\ip) )} \\\\\n",
    "   & = & \\sum_{i=1}^m { \\y^\\ip \\log(p(\\hat{y}^\\ip  = 1  | \\x^\\ip))  + (1-\\y^\\ip )\\log( 1 - p(\\hat{y}^\\ip  = 1  | \\x^\\ip) )} \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You should recognize the (negative of) the Log Likelhood as the Binary Cross Entropy loss.\n",
    "\n",
    "So maximizing the log likelihood minimizes the Binary Cross Entropy Loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Log Likelihood of Linear models with normal errors\n",
    "\n",
    "Our Linear models are of the form\n",
    "\n",
    "$$\n",
    "\\hat{\\y}^\\ip = \\Theta^T \\cdot \\x^\\ip + \\epsilon\n",
    "$$\n",
    "where $\\epsilon \\in \\mathcal{N}(0,\\sigma)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The $\\epsilon$ can be interpretted in either of two ways\n",
    "- as an approximation error (inability to fit data exactly)\n",
    "- measurement error, as explained above\n",
    "   \n",
    "So our prediction  $\\hat{\\y}^\\ip = \\pr{\\hat{\\y}^\\ip | \\x^\\ip}$\n",
    "$$\n",
    "\\hat{\\y}^\\ip = \\Theta^T \\cdot \\x^\\ip + \\epsilon\n",
    "$$\n",
    "becomes a Normal random variable with\n",
    "mean $\\mu = \\Theta^T \\cdot \\x^\\ip$ and standard deviation $\\sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Substituting the formula for Normal distribution, the conditional probability of $\\hat{\\y}^\\ip$ given $\\x^\\ip$ is\n",
    "\n",
    "$$\n",
    "\\begin{array}[llll] \\\\\n",
    "p(\\hat{\\y}^\\ip | \\x^\\ip) & = & \\frac{1}{\\sigma \\sqrt(2\\pi)} \\exp(- \\frac{(\\hat{\\y}^\\ip - \\mu)^2}{2\\sigma}) & \\text{def. of Normal} \\\\\n",
    "    & \\propto &\\exp(- \\frac{(\\hat{\\y}^\\ip - \\Theta^T \\cdot \\x^\\ip)^2}{2 \\sigma}) & \\text{def. of }\\mu \\\\  \n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Likelihood of the training set, given this model of the conditional probability,\n",
    "is just the product over the training set of $p(\\hat{\\y}^\\ip | \\x^\\ip)$:\n",
    "$$\n",
    "\\mathbb{L}_{\\text{model}} = \\prod_{i=1}^m {p(\\hat{\\y}^\\ip | \\x^\\ip)}\n",
    "$$\n",
    "and the Log Likelihood is\n",
    "$$\n",
    "\\begin{array}[llll] \\\\\n",
    "\\mathbb{l}_{\\text{model}} & = & \\log( \\prod_{i=1}^m {p(\\hat{\\y}^\\ip | \\x^\\ip)}) \\\\\n",
    "& = &  \\sum_{i=1}^m {\\log( p(\\hat{\\y}^\\ip | \\x^\\ip) )} \\\\\n",
    "& = &  \\sum_{i=1}^m { - \\frac{(\\hat{\\y}^\\ip - \\Theta^T \\cdot \\x^\\ip)^2}{2 \\sigma}} \\\\\n",
    "& = &   - \\frac{1}{2 \\sigma} \\sum_{i=1}^m { {(\\hat{\\y}^\\ip - \\Theta^T \\cdot \\x^\\ip)^2}}\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You should recognize the (negative of) the Log Likelihood as the Mean Squared Error (MSE).\n",
    "\n",
    "So maximizing the log likelihood minimizes the MSE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Complex loss functions: multiple objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Regularization objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cost functions for Deep Learning: Preview\n",
    "\n",
    "The Cost functions for Classical Machine Learning were perhaps motivated by the desire for closed form solutions.\n",
    "\n",
    "In Deep Learning, the optimization is typically solved via search.\n",
    "\n",
    "This opens the possibilities of complex cost functions that don't require closed form solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As we will see in the Deep Learning part of this course, the key part of solving a task\n",
    "is in *defining* a cost function that mirrors the task's objective.\n",
    "\n",
    "Thus, many cost functions are problem specific and often quite creative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cool cost functions: Neural Style Transfer\n",
    "Neural Style Transfer\n",
    "\n",
    "Given \n",
    "- a \"Content\" Image that you want to transform\n",
    "- a \"Style\" Image (e.g., Van Gogh \"Starry Night\")\n",
    "- Generate a New image that is the Content image redrawn in the style of the Style Image\n",
    "    - [Gatys: A Neural Algorithm for Style](https://arxiv.org/abs/1508.06576)\n",
    "    - [Fast Neural Style Transfer](https://github.com/jcjohnson/fast-neural-style)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Content image\n",
    "<img src=images/chicago.jpg width=500> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Style image\n",
    "<img src=images/starry_night_crop.jpg width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Generated image\n",
    "<img src=images/chicago_starry_night.jpg width=500> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Cost function\n",
    "\n",
    "Definitions:\n",
    "- Style image, represented as a vector of pixels $\\vec{a}$\n",
    "- Content image, represented as a vector of pixels $\\vec{p}$\n",
    "- Generated image, represented as a vector of pixels $\\vec{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Loss function (which we want to minimize by varying $\\vec{x}$) has two parts\n",
    "\n",
    "$$\n",
    "\\text{L} = \\text{L}_{\\text{content}}(\\vec{p}, \\vec{x}) + \\text{L}_{\\text{style}}(\\vec{a}, \\vec{x})\n",
    "$$\n",
    "\n",
    "- a Content Loss\n",
    "    - measure of how different the generated image $\\vec{x}$ is from Content image  $\\vec{p}$\n",
    "- a Style Loss\n",
    "    - measure of how different the \"style\" of generated $\\vec{x}$ is from style of Style image $\\vec{a}$\n",
    "    \n",
    "\n",
    "Key: defining what is \"style\" and similarity of style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370.594px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

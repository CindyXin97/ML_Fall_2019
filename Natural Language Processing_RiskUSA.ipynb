{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{\\tp}{\\mathbf{{(t)}}}\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro `_latex_std_` created. To execute, type its name (without quotes).\n",
      "=== Macro contents: ===\n",
      "get_ipython().run_line_magic('run', 'Latex_macros.ipynb')\n",
      " "
     ]
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "$\n",
    "\\newcommand{\\V}{\\mathbf{V}}\n",
    "\\newcommand{\\v}{\\mathbf{v}}\n",
    "\\newcommand{\\wp}{\\mathbf{{(w)}}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Risk Manager/Quant world\n",
    "\n",
    "- Our job is to deliver insights\n",
    "    - to portfolio managers\n",
    "    - traders\n",
    "    - management\n",
    "- Insights\n",
    "    - into our portfolio\n",
    "    - into the market\n",
    "    - into trader behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Our insights are derived from *data analysis*\n",
    "- price histories\n",
    "- discovery of common factors\n",
    "- trade performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And lead to actionable decisions\n",
    "- individual security buy/sell recommendations\n",
    "- which sectors to over/under weight\n",
    "- where (and to whom) to allocate capital\n",
    "- what factors influence price\n",
    "- under what conditions is a trader successful ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The raw material for our analytics has, historically, been numeric\n",
    "- But there is a lot of non-numeric data that may aid prediction\n",
    "    - images\n",
    "    - speech\n",
    "    -text\n",
    "    \n",
    "*Natural Language Processing* is the set of tools/techniques that facilitate using text as raw material."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The world of text\n",
    "\n",
    "- SEC filing\n",
    "- analyst reports\n",
    "- news articles\n",
    "- tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There is a sequence of steps in dealing with text.\n",
    "\n",
    "We will highlight the challenges (and some solutions) in creating an NLP pipeline/workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Our goals:\n",
    "- Learn to analyze text\n",
    "- Learn to *augment* numerical analyses with inputs derived from text\n",
    "- Introduce open-source tools to enable us to deal with the challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Challenges: Overview\n",
    "\n",
    "We briefly introduce the major components of an NLP pipeline.\n",
    "\n",
    "After a brief intro to all, we will do a deeper dive on each.\n",
    "\n",
    "- Obtaining text\n",
    "    - web scraping\n",
    "- Word recognition  \n",
    "     - tokenization, parsing\n",
    "    - part of speech\n",
    "    - entity recognition\n",
    "- Representation of words\n",
    "- Representation/meaning of sentences/documents\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Challenge 1: Obtaining text\n",
    "\n",
    "Documents are rarely presented to as a simple collection of words\n",
    "- Structure documents\n",
    "    - Web pages\n",
    "    - Bloomberg articles\n",
    "- Markup\n",
    "    - needs to be removed (HTML) to extract the \"text\" parts\n",
    "    \n",
    "We will briefly describe Web-Scraping as a partial solution to this challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Challenge 2: Word recognition\n",
    "\n",
    "Text is more than words.\n",
    "- word separators, punctuation\n",
    "- Markup\n",
    "\n",
    "Even after we have removed markup\n",
    "- we have punctuation\n",
    "    - Divides sentences, so has semantic meaning, not just syntactic\n",
    "- Parsing strings into words\n",
    "    - tokenization\n",
    "- Word variants\n",
    "    - plural\n",
    "    - Capitlization\n",
    "    \n",
    "We will briefly describe some tools for processing text into words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Challenge 3: Word representation\n",
    "\n",
    "We motivate our study of NLP by enumerating key issues in dealing with text.\n",
    "\n",
    "**Notation**\n",
    "\n",
    "- Let $\\V$ be the **vocabulary**\n",
    "    - the ordered collection of distinct words in our universe\n",
    "    - the $i^{th}$ word is denoted $\\V_i$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Text is not a number\n",
    "\n",
    "- Models/algorithms deal with numbers: text is not a number !\n",
    "\n",
    "How hard can it be to turn a word into a number ?\n",
    "- i.e., assign an integer $I_i$ to word $\\V_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The naive method of arbitrarily mapping a word to an integer won't work.\n",
    "- it will imply an order and a magnitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Example**\n",
    "\n",
    "Linear regression:\n",
    "$$\n",
    "\\y = \\Theta^T \\x\n",
    "$$\n",
    "\n",
    "Predict $\\y$ given feature vector (attributes) $\\x$\n",
    "- by learning parameters $\\Theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- But our assignment of integers to words was arbitrary\n",
    "    - multiply all integers by 10\n",
    "    - permute the assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Suppose one feature $\\x_j$ is the numeric encoding of a word\n",
    "- if \"apple\" is encoded by integer 100 rather than integer 10\n",
    "    - $I_{\\text{apple}} = 100$ versus $I_{\\text{apple}} = 10$\n",
    "    - does it have 10 times the impact on $\\y$ ?\n",
    "    - according to the equation\n",
    "        - impact is $\\Theta_j * \\x_j$\n",
    "        - so impact is proportional to the numeric value of the word\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What about the differential impact of $\\x_j$ being different words ?\n",
    "- $I_{\\text{apple}} = 100$ versus $I_{\\text{orange}} = 10$\n",
    "- Are apples 10 times more important than oranges ?\n",
    "    - impact is $\\Theta_j * \\x_j$ so, yes, that's what it implies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Words are categorical variables, not ordinal**\n",
    "\n",
    "- Ordinal\n",
    "    - has magnitude\n",
    "    - ordering relationships ($\\lt, =, \\gt$) is defined\n",
    "- Categorical\n",
    "    - no magnitude\n",
    "    - nor ordering\n",
    "        - Is the word \"apples\" > \"apple\" ?\n",
    "        - Which is greater: \"apples\" or \"oranges\" ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### There are lots of words !\n",
    "\n",
    "\n",
    "Representing categorical variables\n",
    "- dummy/indicator variables\n",
    "\n",
    "Create a binary indicator variable *for each word* in the vocabulary\n",
    "- $\\text{Is}_\\text{apple}, \\text{Is}_\\text{orange}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Linear Regression deals with Categoricals quite well\n",
    "- the contribution to  prediction $\\y$ is\n",
    "    - 0 if \"apple\" not present as a feature\n",
    "    - increases by $\\Theta_j$ if \"apple\" is word $j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Generalization: One Hot Encoding (OHE)\n",
    "    - vector of length equal to vocabulary size $|V|$\n",
    "    - all elements of vector 0 except for position $j$, for the $j^th$ word in vocabulary\n",
    "    - word $i$ represented by a vector $\\v$\n",
    "        - $\\v_j = 0, \\forall j \\ne i$\n",
    "        - $\\v_i = 1$\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let $\\v_w$ denote the One Hot Encoding of word $w$ (e.g., \"apple\").\n",
    "\n",
    "The length of $\\v_w$ is $| \\V |$, the number of words in the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Easy enough to convert each word to a categorical variable.\n",
    "\n",
    "Are we done ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A vocabulary easily has thousands (more like tens of thousands) or words\n",
    "- OHE are long !\n",
    "- Stated another way:\n",
    "    - the number of independent variables in our regression equation is $|V|$\n",
    "    \n",
    "It is unwieldy to deal with thousands of categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What's in a word ?\n",
    "\n",
    "What is \"orange\"\n",
    "- a color ?\n",
    "- a fruit ?\n",
    "- a brand name ?\n",
    "\n",
    "The string of characters is not sufficient to convey full meaning !\n",
    "- How do we discern \"which orange\" we mean ?\n",
    "- The distinct meanings increases the size of $\\V$ even more !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Parts of speech\n",
    "- Named Entity Recognition\n",
    "    - recognizing a word as an instance of a \"concept\"\n",
    "        - can replace the concrete word by the name of the concept in many cases\n",
    "            - recognizing Apple as company\n",
    "            - recognizing \"11/04/2019\" as a date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Challenge 4:  Sentence/Document representation\n",
    "\n",
    "Now that we addressed th represenation of words: how to we represent a sentence/document ?\n",
    "\n",
    "Let $\\w$ be the array of $n$ words in a sentence.\n",
    "\n",
    "The simplest way to represent a sentence would be the *set* of word representations.\n",
    "\n",
    "This turns out to be impractical for the same reason that OHE representaion of words is impractical: size.\n",
    "- Let $\\text{rep}(\\w_i)$ be a represention of word $i$\n",
    "- $\\{ \\text{rep}(\\w_1), \\text{rep}(\\w_2), \\ldots, \\text{rep}(\\w_n) \\}$\n",
    "- But if $\\text{rep}(\\w_i)$ is a OHE\n",
    "    - total size $n * |V|$.  Huge !\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Sentences are of varying length\n",
    "\n",
    "There is another issue with the array of words:\n",
    "- sentences are of varying length\n",
    "\n",
    "It would be highly undesirable if our algorithm was in any way influenced by the length of the sentence.\n",
    "\n",
    "Moreover: many models can only deal with *fixed length* inputs\n",
    "- pad short sequences\n",
    "- truncate long sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Sentences/Documents are sequences not sets\n",
    "\n",
    "To state the obvious: the ordering of words in a sentence is important.\n",
    "\n",
    "- [ ...,  \"not\", \"like\", ... ] vs [ \"like\", \"me\", ..., \"not\", \"anyone, \"else\" ]\n",
    "    - consecutive \"not like\" is negative; independent is not necessarilty positive/negative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So our sentence representation must respect order.\n",
    "\n",
    "Thus, an array of word representations is preferable.\n",
    "\n",
    "$[ \\text{rep}(\\w_1), \\text{rep}(\\w_2), \\ldots, \\text{rep}(\\w_n) ]$\n",
    "\n",
    "The disadvantage, compared to a set\n",
    "- the length of the sentence reprsentation depends on number of words in sentence\n",
    "    - rather than number of *unique* words\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Subsequences/n-grams\n",
    "\n",
    "Words in isolation don't necesarily convey full meaning of a concept:\n",
    "- \"New York City\" versus [ \"New\", \"York\", \"City\" ]\n",
    "- A bigram is a two token sequence that encodes a single concept\n",
    "- A trigram is a three token sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dealing with the challenges\n",
    "\n",
    "We now describe how to deal with each challenge.\n",
    "\n",
    "It's important to note that many choices are *not independent* of one another\n",
    "- later stages of the pipeline are impacted by earlier choices\n",
    "- adds complexity; no simple answer for each step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Response: Obtaining text challenge\n",
    "\n",
    "Text data rarely comes to you in a neatly wrapped package.\n",
    "\n",
    "Your first challenge is obtaining it\n",
    "- PDF files\n",
    "- Web pages\n",
    "\n",
    "The data is typically in some structured form that includes both the text (that you want)\n",
    "and *mark-down* (e.g., HTML, which gives meaning to some text, but which you ultimately don't want).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Web scaping\n",
    "\n",
    "**The problem**\n",
    "\n",
    "Here is a typical web page, as rendered in your browswer\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Web page</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RiskUSA_web.png\" width=800></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Suppose we want the abstract, time and speakers for each session\n",
    "- in this case, we preserve some of the structure\n",
    "\n",
    "Here's what the web page's source looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Page source</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RiskUSA_html.png\" width=1000></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- It is highly structured\n",
    "    - Good ! Can extract semantic concepts\n",
    "        - time\n",
    "        - speaker\n",
    "    - Bad ! How do I deal with this ?  I only want the text !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Solution**\n",
    "\n",
    "A Web-Scraper is a tool that can parse Page Source (HTML/XML) into units\n",
    "- preserve structure\n",
    "- remove markup\n",
    "- left with text !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Beautiful Soup: a tool for document scraping\n",
    "\n",
    "Beautiful Soup is a very popular, open-source tool for scraping.\n",
    " \n",
    "[https://www.crummy.com/software/BeautifulSoup/](https://www.crummy.com/software/BeautifulSoup/#Download)\n",
    "\n",
    "If you're serious about text, you should\n",
    "- invest in learning a tool set\n",
    "- **build your own** higher level tools for common tasks\n",
    "\n",
    "This is a tool for programmers, not a GUI.\n",
    "\n",
    "Here's some sample HTML source (taken from the documentation):\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<html>\n",
    " <head>\n",
    "  <title>\n",
    "   Page title\n",
    "  </title>\n",
    " </head>\n",
    " <body>\n",
    "  <p id=\"firstpara\" align=\"center\">\n",
    "   This is paragraph\n",
    "   <b>\n",
    "    one\n",
    "   </b>\n",
    "   .\n",
    "  </p>\n",
    "  <p id=\"secondpara\" align=\"blah\">\n",
    "   This is paragraph\n",
    "   <b>\n",
    "    two\n",
    "   </b>\n",
    "   .\n",
    "  </p>\n",
    " </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And some Python code to navigate/extract contents.\n",
    "\n",
    "First: some imports and loading the document into BS."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "from BeautifulSoup import BeautifulSoup          # For processing HTML\n",
    "from BeautifulSoup import BeautifulStoneSoup     # For processing XML\n",
    "import BeautifulSoup                             # To get everything\n",
    "\n",
    "import re\n",
    "\n",
    "# Here is the page source, already loaded into a string -- for convenience of presentation only\n",
    "doc = ['<html><head><title>Page title</title></head>',\n",
    "       '<body><p id=\"firstpara\" align=\"center\">This is paragraph <b>one</b>.',\n",
    "       '<p id=\"secondpara\" align=\"blah\">This is paragraph <b>two</b>.',\n",
    "       '</html>']\n",
    "doc = ''.join(doc)\n",
    "\n",
    "soup = BeautifulSoup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's walk through the document.\n",
    "\n",
    "What's the first element ?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "soup.contents[0].name\n",
    "# u'html'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The first element (html) is *itself* structured.\n",
    "\n",
    "What is the first part (of the first element) ?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "soup.contents[0].contents[0].name\n",
    "# u'head'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What about the first part of the next element (i.e., sibling of `html`)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "head.nextSibling.contents[0]\n",
    "# <p id=\"firstpara\" align=\"center\">This is paragraph <b>one</b>.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Response: Word recognition challenge\n",
    "\n",
    "## Normalization\n",
    "\n",
    "Put the raw words into a standard form (after removing markdown) by:\n",
    "\n",
    "- tokenization\n",
    "    - isolating strings of characters into \"word\" tokens\n",
    "- word normalization\n",
    "    - case\n",
    "    - tense\n",
    "    - stemming\n",
    "    - lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Tokenization\n",
    "\n",
    "    \n",
    "This is not terribly complicated, nor is it completely trivial.\n",
    "\n",
    "It usually involves some decent amount of pattern matching for which *regular expressions* are very useful.\n",
    "\n",
    "Some questions\n",
    "- how to separate words\n",
    "    - word separators: space, line-end, punctuation\n",
    "        - is language dependent: Mandarin, Japanese don't use spaces\n",
    "        - punctuation may carry meaning\n",
    "            - \"Really ?\", \"Really !\", \"Really !!!\"\n",
    "    - special cases\n",
    "        - punctuation as part of a word: U.S.A.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We won't dwell on this and will just use tokenizers from some standard libaries.\n",
    "\n",
    "- `sklearn`\n",
    "- `keras`\n",
    "- `nltk`\n",
    "- `spaCy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word normalization\n",
    "\n",
    "We want to put words in a \"standard\" (canonical) form.\n",
    "\n",
    "This can be problem dependent and **choices affect the entire downstream flow !**\n",
    "- machine translation problems may benefit from minimal normalization\n",
    "- text classification may benefit by reducing variation\n",
    "\n",
    "Some issues\n",
    "- Case sensitive or not ? Apple/apple\n",
    "- U.S.A , USA, U.S, US $\\mapsto$ US\n",
    "- tense\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Case folding\n",
    "- Do we turn everything into lower case ?\n",
    "    - Case sometimes conveys meaning\n",
    "        - Proper names: Bill vs bill\n",
    "        - Start of sentence indicator\n",
    "        - Indicates emotion: NO WAY\n",
    "    - where did the text come from ? Text messages often in all lower case\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Stemming\n",
    "- plural/singular\n",
    "    - car/cars $\\mapsto$ car\n",
    "- tense\n",
    "    - walk, walks, walking, walked $\\mapsto$ walk\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Lemmatization\n",
    "More complex version of stemming: determines whether words have the same root\n",
    "- am, is, are $\\mapsto$ be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Stop words\n",
    "\n",
    "*Stop words* are words with low information content: \"a\", \"the\"\n",
    "\n",
    "Task specific (so no simple rule works for all tasks)\n",
    "- remove for\n",
    "    - classification\n",
    "- don't remove for\n",
    "    - translation\n",
    "    - question answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Sentence segmentation\n",
    "\n",
    "Each sentence (should) represent a single thought.\n",
    "\n",
    "Where does the sentence end ?\n",
    "\n",
    "Not always clear\n",
    "- role of semi-colon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tagging/POS\n",
    "\n",
    "For some problems, identifying parts of speech  POS may be relevant.\n",
    "- Bill (the person): proper noun\n",
    "- Bill me later: verb, start of sentence\n",
    "- send me the bill: noun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "*Entity recognition*: a word as an instance of a \"concept\"; replace the concrete word with the concept.\n",
    "\n",
    "- Alice, Bob are instances of the  `Person` entity\n",
    "    - does your task only need to know that the subject is a Person or a specific person ?\n",
    "    - Ken Perry: multi-word proper name, `Person` entity\n",
    "        - \"In his talk Ken Perry said ..\"\n",
    "        - \"In his talk PERSON said ..\"\n",
    "\n",
    "- 11/04/2019 is instance of a `Date` entity\n",
    "- Google is instance of `Organization` entity\n",
    "\n",
    "\n",
    "Note that recognizing entities may be affected by prior decisions: eliminating case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Toolkits for Natural Language Processing\n",
    "\n",
    "Two popular toolkits to solve word-recognition (and more) problems\n",
    "- nltk\n",
    "- spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### NLTK\n",
    "\n",
    "[https://www.nltk.org/](https://www.nltk.org/)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's convert a sentence into words (tokenization).\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " import nltk\n",
    " sentence = \"\"\"At eight o'clock on Thursday morning\n",
    "... Arthur didn't feel very good.\"\"\"\n",
    " \n",
    "tokens = nltk.word_tokenize(sentence)\n",
    " tokens\n",
    "['At', 'eight', \"o'clock\", 'on', 'Thursday', 'morning',\n",
    "'Arthur', 'did', \"n't\", 'feel', 'very', 'good', '.']\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Lest you think that tokenization is trivial: notice the sophisticated way it handled\n",
    "- \"o'clock\"\n",
    "    - recognized as single word\n",
    "- \"didn't\"\n",
    "    - recognized as contraction of \"did\" and \"not\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's identify parts of speech.  We might use these to derive meaning."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "tagged = nltk.pos_tag(tokens)\n",
    " tagged[0:6]\n",
    "[('At', 'IN'), ('eight', 'CD'), (\"o'clock\", 'JJ'), ('on', 'IN'),\n",
    "('Thursday', 'NNP'), ('morning', 'NN')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Who knew their were so many parts of speech !\n",
    "- \"8\" is a CD: cardinal digit \n",
    "- \"Thursday\" is an NNP: proper noun, singular (sarah)  \n",
    "- \"morning\" is an NN: noun, singular (cat, tree) \n",
    "- \"At\", \"on\" are IN: preposition/subordinating conjunction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### spaCy \n",
    "\n",
    "[https://spacy.io/](https://spacy.io/)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Response: Word representation challenge\n",
    "\n",
    "Congratulations ! You have converted strings of characters  \"words\".\n",
    "\n",
    "These \"words\" are organized into larger collections\n",
    "\n",
    "- a sentence is a *sequence* of words.\n",
    "- a document is a sequence of sentences\n",
    "\n",
    "Before we even get to how ML deals with sequences, let's talk about how\n",
    "we can turn individual words to numeric vectors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Let $\\V$ be the **vocabulary**\n",
    "    - the collection of distinct words in our universe\n",
    "    - denoted by a vector $\\V$ so that word $i$ is denoted $\\V_i$\n",
    "    \n",
    "Each word $v \\in \\V$ needs to be turned into a *feature vector* $\\v$: an array of numbers encoding the word.\n",
    "\n",
    "Note that that $\\v$ is a vector (perhaps of length 1) because a word may turn out to be encoded\n",
    "by many features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Aside**: Expanded Vocabularies\n",
    "\n",
    "It is not uncommon to expand the Vocabulary in order to capture some semantic information:\n",
    "- tokens to denote position: $\\text{<START>}, \\text{<END>}$\n",
    "- n-grams\n",
    "    - pairs, triples, etc. of consecutive words\n",
    "        - in this case may appear in each of these tuples\n",
    "        \n",
    "So when we refer to Vocabulary, this sometimes includes more than just the raw words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Representing words by integers\n",
    "\n",
    "Perhaps the simplest representation of $v$ is as the index in $\\V$ of $v$\n",
    "- $\\v = [ i ]$ where $\\V_i = v$\n",
    "\n",
    "Properly speaking, words are most likely *categorical* (not *ordinal*, i.e., no ordering relationship between words).\n",
    "\n",
    "So one should not make use of the ordinal relationship of the integer representation.\n",
    "\n",
    "- [`sklearn.preprocessing.LabelEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sparse Represention of words: One Hot Encoding (OHE)\n",
    "\n",
    "The proper way to represent categorical variable $v$ is via One Hot Encoding.\n",
    "- $\\v$ is of length $|| \\V ||$\n",
    "- $\\v_i = 1$ for the $i$ such that $\\V_i = v$\n",
    "- $\\v_j = 0$ for $j \\ne i$\n",
    "\n",
    "- [`sklearn.preprocessing.OneHotEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder)\n",
    "\n",
    "OHE is a *sparse* representation\n",
    "- length of $\\v$ is $|| \\V ||$, yet only a single non-zero element\n",
    "- problematic for large $\\V$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dense representation of words: Embeddings\n",
    "\n",
    "Sparse encodings above essentially reduced the representation of a word to a single active feature.\n",
    "\n",
    "This is called a *discrete* representation.\n",
    "\n",
    "Categorical representations have a major drawback\n",
    "- there is no meaningful metric of \"distance\" between the representation of words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let \n",
    "\n",
    "$$\n",
    "\\text{OHE}(w)\n",
    "$$\n",
    "\n",
    "denote the One Hot Encoding of word $w$.\n",
    "\n",
    "Using dot product (cosine similarity) as a measure of similarity\n",
    "\n",
    "| word   | OHE(word) | Similarity |\n",
    "| ---    | ---       | :---:        |\n",
    "| dog   | [1,0,0,0]   | OHE(word) $\\cdot$ OHE(dog)  = 1  |\n",
    "| dogs  | [0,1,0,0]   | OHE(word) $\\cdot$ OHE(dog)  = 0  |\n",
    "| cat   | [0,0,1,0]   | OHE(word) $\\cdot$ OHE(dog)  = 0  |\n",
    "| apple | [0,0,0,1]   | OHE(word) $\\cdot$ OHE(dog)  = 0  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Each pair of distinct words has 0 similarity\n",
    "- no recognition of plural form\n",
    "- no recognition of commonality (pets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "However, it's possible that there are many \"dimensions\" to a word, for example\n",
    "- singular/plural\n",
    "- entity type, e.g., Person\n",
    "- positive/negative\n",
    "\n",
    "Thus it is not unreasonable to represent a word as a vector of features where there is\n",
    "a numeric strength associated with the feature.\n",
    "\n",
    "Ideally the features would be indepenent?\n",
    "\n",
    "This is called a *continuous* word representation.\n",
    "\n",
    "In the section on Embeddings, we will learn how to automatically construct dense vector representations\n",
    "in a way that capture properties of the words in $\\V$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evaluating Dense vector representations\n",
    "\n",
    "**Word analogies**\n",
    "\n",
    "king:man :: ? : queen\n",
    "\n",
    "\n",
    "Let\n",
    "- $\\v_w$ be the dense vector for word $w$\n",
    "- $d(\\v_{w}, \\v_{w'})$ be some measure of the distance between the two vectors $\\v_{w}, \\v_{w'}$\n",
    "    - e.g., ( 1- cosine similarity )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Using the distance metric,  define the set of words in vocabulary $\\V$ that are \"closest\" to a word $w$.\n",
    "\n",
    "Let\n",
    "- $\\text{wv}_{n',d}(\\v_w)$ be the dense vectors of the $n'$ words in $\\V$ closest to word $w$\n",
    "$$\n",
    "\\text{wv}_{n',d}(\\v_w) = \\{ \\v_{w'} | \\text{rank}_V( d(\\v_{w}, \\v_{w'}) ) \\le n' \\}\n",
    "$$\n",
    "- $N_{n',d}(w)$ be the set of $n'$ words in $\\V$ closest in distance metric $d$ to word $w$\n",
    "\n",
    "\n",
    "$$\n",
    "N_{n',d}(w) = \\{ w' | w' \\in \\text{wv}_{n',d}(\\v_w) \\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can define approximate equality of two words $w, w'$ if they are among the closest words \n",
    "\n",
    "$$\n",
    "w \\approx_{n',d} w' \\; \\; \\text{if } \\w' \\in N_{n',d}(w) \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finally, we can define word analogies:\n",
    "\n",
    "a:b :: c:d\n",
    "\n",
    "means\n",
    "\n",
    "$$\n",
    "\\v_a - \\v_b  \\approx_{n',d}  \\v_c - \\v_d \n",
    "$$\n",
    "\n",
    "So to solve the word analogy for $c$:\n",
    "$$\n",
    "\\v_c \\approx_{n',d}  \\v_a - \\v_b + \\v_d\n",
    "$$\n",
    "\n",
    "To be concrete:\n",
    "$$\n",
    "\\v_\\text{king} - \\v_\\text{man} + \\v_\\text{woman} \\approx_{n',d} \\v_\\text{queen}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Why does adding 2 word vectors work\n",
    "- Mikolov\n",
    "    - Vector for a word reflects its context\n",
    "    - Vector is log probability\n",
    "        - so sum of log probabilities is log of product of probabilities\n",
    "            - product is like a logical \"and\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### GloVe: Pre-trained embeddings\n",
    "\n",
    "GloVe is a family of word embeddings that have been trained on large corpra\n",
    "- GloVe6b\n",
    "    - Trained on 6 Billion tokens\n",
    "    - 400K words\n",
    "    - Corpus:  Wikipedia (2014) + GigaWord5 (version 5, news wires 1994-2010)\n",
    "    - Many different dense vector lengths to choose from\n",
    "        - 50, 100, 200, 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will illustrate the power of word embeddings using GloVe6b vectors of length $100$.\n",
    "\n",
    "$$\n",
    "\\begin{array}[llllll]\\\\\n",
    "\\text{king- man + woman} &  \\approx_{n',d} & \\text{queen } \\\\\n",
    "\\text{man - boy + girl} &  \\approx_{n',d} & \\text{woman } \\\\\n",
    "\\text{Paris - France + Germany} &  \\approx_{n',d} & \\text{Berlin } \\\\\n",
    "\\text{Einstein - science + art} &  \\approx_{n',d} & \\text{Picasso} \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "You can see that the dense vectors seem to encode \"concept\", that we can manipulate mathematically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You may discover some unintended bias\n",
    "\n",
    "$$\n",
    "\\begin{array}[llllll]\\\\\n",
    "\\text{doctor - man + woman} &  \\approx_{n',d} & \\text{nurse } \\\\\n",
    "\\text{mechanic  - man + woman} &  \\approx_{n',d} & \\text{teacher } \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Domain specific embeddings\n",
    "\n",
    "Do we speak Wikipedia English in this room ?\n",
    "\n",
    "Here are the neighborhoods of some financial terms:\n",
    "\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "N(\\text{bull}) & =  & [ \\text{cow, elephant, dog, wolf, pit, bear, rider, lion, horse}] \\\\\n",
    "N(\\text{short}) & =  & [ \\text{rather, instead, making, time, though, well, longer, shorter, long}] \\\\\n",
    "N(\\text{strike}) & =  & [ \\text{workers, struck, action, blow, striking, protest, stoppage, walkout, strikes}] \\\\\n",
    "N(\\text{FX}) & =  & [ \\text{showtime, cnbc, ff, nickelodeon, hbo, wb, cw, vh1}] \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "It may be desirable to create word embeddings on a narrow (domain specific) corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Creating embeddings from word prediction problems\n",
    "\n",
    "\n",
    "Word embeddings can be obtained as a by-product of a *word prediction* problem.\n",
    "\n",
    "Let $\\w$ be the sequence of $n$ words $[ \\w_{(0)}, \\w_{(1)}, \\ldots, \\w_{(n)} ] $\n",
    "\n",
    "A *word prediction* is a mapping from input to a probability distribtuion over vocabulary $\\V$\n",
    "- a vector of length $|\\V|$ each value being in the range $[0,1]$ and summing to $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here are some simple word prediction problems:\n",
    "$$\n",
    "\\begin{array}[lll]\\\n",
    "\\text{predict next word from context}  & \\pr{\\w_{(i)} | \\w_{(i-1)}, \\ldots, \\w_{(i-m)} } \\\\\n",
    "\\text{predict a surrounding word}      & \\pr{\\w_{(i')} | \\w_{(i)} } & (i-m) \\le i' \\le (i + m) \\\\\n",
    "\\text{predict center word from conext} & \\pr{ \\w_{(i)} | [ \\w_{(i-m)}, \\ldots, \\w_{(i-1)}, \\w_{(i+1)}, \\ldots, \\w_{(i+m)} ] }  & \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# word2vec: concrete example of creating word embeddings\n",
    "\n",
    "\n",
    "## Prediction problem for word2vec\n",
    "\n",
    "word2vec is based on one of two prediction problems.\n",
    "- predict center word given surrounding words as context\n",
    "- precict which words can occur on either side of a given center words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Vocabulary $V$\n",
    "- window $m$ on either side\n",
    "- $k$ length of embedding vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let $\\w$ be the array of $n$ words in a sentence.\n",
    "\n",
    "For convenience we define two pseuduo-words to denote the start/end of the sentence\n",
    "- $\\w_0 = \\text{<START>}$\n",
    "- $\\w_{n+1} =\\text{<END>}$\n",
    "\n",
    "The problems are framed as:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Prediction problem: Predict target word $w_t$ given conditional word $w_c$\n",
    "- $p(w_t|w_c) $\n",
    "\n",
    "The first prediction problems is called *Skip gram*\n",
    "- One (center word) to many (surrounding words)\n",
    "    - $w_c$ conditional word (input word) is center word\n",
    "    - $w_t$ is one of the \"context\" words in a window of size $n$ from the center word\n",
    "\n",
    "    - So given sentence fragment of size $2m + 1$:\n",
    "        $$w_{i-m}, \\ldots, w_{i-1}, w_c, w_{i+1}, \\ldots, w_{i+m}$$\n",
    "        - training (input, label) pairs:\n",
    "        $$\\{ (w_c, w_j) \\; | \\; j \\in \\{ (i-m), \\dots, (i+m) \\}- \\{i\\} \\}$$\n",
    "    - $w_c$ is a one-hot vector of length $|V|$\n",
    "    - $w_j$ is a one-hot vector of length $|V|$: probability vector\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The second prediction problem is called *CBOW*\n",
    "- many (surrounding words) to one (center word)\n",
    "    - $w_c$: conditional words are words surrounding $\\w_i$\n",
    "    - $w_t = w_i$\n",
    "    - So given sentence fragment of size $2m + 1$:\n",
    "        $$w_{i-m}, \\ldots, w_{i-1}, w_t, w_{i+1}, \\ldots, w_{i+m}$$\n",
    "        - training (input, label) pairs:\n",
    "        $$\\{ (\\{ w_j | j \\in \\{ (i-m), \\dots, (i+m) \\}- \\{i\\} \\}, w_i) \\; | \\; \\}$$\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can construct a fairly simple Neural Network (NN) to\n",
    "- solve the word prediction task\n",
    "- obtain dense vector embeddings (of length $k$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Word prediction: Neural Net</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/w2v_word_prediction.jpg\" width=800></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The NN solves a maximization problem\n",
    "\n",
    "- Maximize average log probability over the $T$ examples in training set:\n",
    "\n",
    "$$\\mathcal{U} = \\frac{1}{T} \\sum_{t=1}^T { \\sum_{-m \\le j \\le m, j \\ne 0} { \\log( p(w_{t+j}|w_t) )} } $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Maximiziation problem; Find $C, \\Theta$ that maximizes the (log) likelihood\n",
    "\n",
    "$$\n",
    "C, \\Theta = \\argmax{C,\\Theta} { \\mathcal{U} }\n",
    "$$\n",
    "\n",
    "In words:\n",
    "- solve for the matrix C, and logistic regression parameters $\\Theta$ that maximizes probability of predicting correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note\n",
    "- dimension of $C: ( |V| \\times k )$\n",
    "    - row $i$: maps $\\w_i$ to a dense vector of length $k$\n",
    "- dimension of $\\Theta: (|V| \\times k)$\n",
    "    - row $i$: regression weights for binary classification of target being $\\w_i$\n",
    "    \n",
    "- Construct $E$ after the fact:\n",
    "    - the $i\\text{-th}$ row of $E$ is $\\theta_i$\n",
    "\n",
    "So both $C$ and $\\Theta$ can be used to create the dense vectors of length $k$\n",
    "- in practice: average $C$ and $\\Theta$\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tricks for training word2vec\n",
    "\n",
    "#### Negative sampling (Noise Contrastive Estimation)\n",
    "\n",
    "The cartoon version of solving the word prediction problem has some computational issues\n",
    "- Softmax denominator involves $|V|$ terms\n",
    "- impractical for large V\n",
    "    - $|V|$ gradients to compute for each (example, distance) from center pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Solution: negative sampling\n",
    "- *approximate* denominator by sampling negative examples\n",
    "    - For each positive example (wrt a binary classifier for a single $w_j \\in V$)\n",
    "    $$\\{ ( [w_c, w_j], \\text{True}) \\; | \\; j \\in \\{ (i-m), \\dots, (i+m) \\}- \\{i\\} \\}$$\n",
    "  \n",
    "    - add $k$ negative examples to training set (can't learn from just positiv examples)\n",
    "$$\\{ ( [w_c, w_{n_j}], \\text{False})\\;  | \\; 1 \\le j \\le k \\} $$\n",
    "    - can choose $w_{n_j}$ at random\n",
    "        - with high probability $w_n$ won't be a neighbor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Sub-sampling\n",
    "\n",
    "- High frequency words don't carray as much information as low frequency words\n",
    "- Sub-sampling: under sample high frequency words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Embeddings: Other topics\n",
    "\n",
    "## Shared Embeddings\n",
    "\n",
    "- joint embedding of words and images\n",
    "    - come up with code for image\n",
    "- joint embedding of two languages\n",
    "    - embedding for English, embedding for Chinese\n",
    "    - know correspondence of some words between languages\n",
    "        - add constraint that Chinise word embedding is close to tranlsated English word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sub-word representations\n",
    "- \"unfortunately\" $\\mapsto$ [\"un\", \"foruntate\", \"ly\" ]\n",
    "    - divide into morphemes\n",
    "    - other divisions possible\n",
    "        - n-gram of letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sentence representation\n",
    "\n",
    "We will present two classes of sentence representation\n",
    "- convert the sequence to a fixed length \n",
    "- models that take sequences as inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Response: Sentence representation challenge\n",
    "\n",
    "## Fixed length representaion of a sentence\n",
    "One way to deal with a sequence $\\w$ of words is to it to a vector $\\x^\\wp$ of **fixed length**.\n",
    "\n",
    "Once the length is fixed, Classical and Deep Learning models taking fixed length inputs\n",
    "can work as usual.\n",
    "\n",
    "Doing so usually involves losing the ordering information.\n",
    "\n",
    "Note that $\\w$ is a sequence (vector) whose elements $\\w_j$  correspond to words.\n",
    "The individual words $\\w_j$ have been given a representation as a vector so $\\w_j$ is\n",
    "a vector, not a scalar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bag of Words (BOW): Pooling\n",
    "\n",
    "We define a *reduction* operation $\\text{CBOW}$ that convert a sequence of length $||w||$ to a fixed length.\n",
    "\n",
    "The fixed length is usually $||V||$ the number of words in the Vocabulary,\n",
    "and hence the reduced form is called a Bag of Words (BOW).\n",
    "\n",
    "There are many operators to achieve the reduction, which we will group under the name *pooling*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Sum/Average\n",
    "$$\n",
    "\\text{CBOW}(\\w) = \\sum_{j=1}^{||\\w ||} {  \\w_j }\n",
    "$$\n",
    "\n",
    "Remember, $w_j$ is a vector (length $l$) so $\\text{CBOW}(\\w)$ is a vector that is\n",
    "the result of element-wise addition of vectors.\n",
    "- $|| \\text{CBOW}(\\w)  || = l$\n",
    "\n",
    "We can easily turn the Sum into an average by dividing by $||w||$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Count vectorization:  a special case of pooling\n",
    "\n",
    "When the representaion of a word $\\w_j$ is a OHE vector of length $||V||$\n",
    "then the Sum reduction returns a vector $\\x^\\wp$ whose $j^{th}$ element $\\x^\\wp_j$\n",
    "is the number of occurences of word $\\V_j$ in sequence $\\w$.\n",
    "\n",
    "This is often called Count Vectorization. (turning $\\w$ into a vector of counts)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### TF-IDF\n",
    "\n",
    "This technique turns a sentence of length $n$ into\n",
    "an array (of length $|V|$) of (word, count) pairs.\n",
    "\n",
    "A vocabulary has many words, some more \"important\" than others in conveying meaning.\n",
    "\n",
    "For example, in the English language \"the\" does not convey much meaning.\n",
    "\n",
    "But the string \"ML\" might be more meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Part of this has to do with frequency of word occurrence: \"the\" occurs so frequently that it doesn't\n",
    "have much meaning.\n",
    "\n",
    "In general purpose English \"ML\" occurs much less often and can be recognized as an abbreviation for\n",
    "Machine Learning.\n",
    "\n",
    "*Term Frequency, Inverse Document Frequency (TF-IDF)* is based on the idea that\n",
    "a word that is *infequent* in the wide corpus but is frequent in a particular document in\n",
    "the corpus is very meaningul in the context of the document.\n",
    "\n",
    "So a document in which \"ML\" occured a disproportionately high (relative to the broad corpus)\n",
    "number of times is likely to indicate that the document is dealing with the subject of Machine Learning.\n",
    "\n",
    "**Note** A similar idea is behind many Web search algorithms (Google)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "TF-IDF is similar to the Count Vectorizer, but with modified counts \n",
    "that are the product of \n",
    "- the frequency of a word within a single document\n",
    "- the inverse of the frequency of the word relative to all documents\n",
    "\n",
    "- $v$ is a word\n",
    "- $d$ is a document (collection of words) in set of documents $D$\n",
    "\n",
    "$$\n",
    "\\begin{array}[llll]\\\\\n",
    "\\text{tf}(v,d) & = & \\text{frequency of word } v \\text{ in document } d & \\text{(Term Frquency)}\\\\\n",
    "\\text{df}(v) & = & \\text{number of documents that contain word } v \\\\\n",
    "\\text{idf}(v) & = & \\log( \\frac{ ||D|| } { \\text{df}(v) } ) + 1 & \\text{Inverse Document Frequency} \\\\\n",
    "\\\\\n",
    "\\text{tf-idf}(v,d) & = & \\text{tf}(v, d) * \\text{idf}(v) \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To be concrete, let's write $l = || \\w_j ||$ to denote the length of the word representation.\n",
    "\n",
    "We will write $\\w_{j,k}$ to refer to element $k$ of the vector representing the $j^{th}$ word $\\w_j$.\n",
    "\n",
    "This should be familiar to anyone using a programming language that represent multi-dimensional\n",
    "arrays as arrays containg elements that are arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Variable length representation (sequence) of a sentence\n",
    "\n",
    "Recurrent Neural Networks (RNN) and variants (LSTM)\n",
    "- take variable length sequences as input\n",
    "- fixed length output (final state)\n",
    "\n",
    "They process the words in the sequence *in order* in order to arrive at the final state\n",
    "- so final state is a representation of the entire sequence\n",
    "- is of fixed length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Some sequence based models are *bi-directional*\n",
    "- part of the model processes the sequence from start to end\n",
    "- part of the model preocesses the sequence from end to start\n",
    "\n",
    "This allows the model to extract greater meaning (particularly with some languages).\n",
    "- English: The Big Apple\n",
    "- French:  Le Pomme Grand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recurrent Neural Networks (and friends)\n",
    "\n",
    "Recurrent Neural Networks (RNN) take *sequences* as inputs.\n",
    "\n",
    "They are ideal for sentences of varying length.\n",
    "\n",
    "There are advanced versions of RNN's\n",
    "- LSTM\n",
    "- GRU\n",
    "\n",
    "We will use RNN generially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_loop.jpg\" width=1000></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You can think of an RNN as a Neural Network in a loop:\n",
    "- performs the same processing on each element of a sequence, one element at a time\n",
    "- *has state*\n",
    "    - encodes information on the prefix of the sequence encountered so far\n",
    "    - so can change behavior on element $t$ of the sequence, depending on elements $1 \\ldots (t-1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is helpful to \"unroll\" the loop and picture the RNN as a (variable length) sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN many to one</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_layer_many_one.jpg\" width=800></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As you can see, the RNN at step $t$\n",
    "- takes as inputs\n",
    "    - $\\x_\\tp$, the $t^th$ element of the sequence\n",
    "    - $\\h_{(t-1)}$, the prior state \n",
    " - creates  $\\h_\\tp$, the new state\n",
    "\n",
    "The final state $\\h_{(n)}$, after processing the sequence of length $n$\n",
    "- is *an encoding of the entire sequence*\n",
    "- may be used as a fixed length input to another model\n",
    "\n",
    "This behavior is a *many to one* mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One can also create a *many to many* mapping\n",
    "- by making each hidden state $\\h_\\tp$ visible\n",
    "- transforms input sequence to an output sequence of equal length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN many to many</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_layer_many_many.jpg\" width=800></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Producing a sequence is useful for several NLP tasks\n",
    "- translating between languages\n",
    "- captioning images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Encoder/Decoder architecture\n",
    "\n",
    "An *Encoder/Decoder* is a two part Neural Network that is applied to many NLP tasks\n",
    "- *Encoder* converts sequence (sentence) into intermediate representation (sequence)\n",
    "- *Decoder* converts intermediate sequence to final sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN Encoder/Decoder</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_Encoder_Decoder.jpg\" width=800></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Examples\n",
    "- Translating between languages\n",
    "    - Encoder: encode source language\n",
    "    - Decoder: decode into target language\n",
    "- Image captioning\n",
    "    - Encoder: encodes a stream of video frames\n",
    "    - Decoder: generate description of the scene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Attention: an enhancement to sequence models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN with Attention</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_Attention.jpg\" width=800></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370.594px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

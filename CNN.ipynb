{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{\\tp}{\\mathbf{{(t)}}}\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro `_latex_std_` created. To execute, type its name (without quotes).\n",
      "=== Macro contents: ===\n",
      "get_ipython().run_line_magic('run', 'Latex_macros.ipynb')\n",
      " "
     ]
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\kernel}{\\mathbf{k}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# My standard magic !  You will see this in almost all my notebooks.\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Reload all modules imported with %aimport\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "import cnn_helper\n",
    "%aimport cnn_helper\n",
    "cnnh = cnn_helper.CNN_Helper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convolutional Neural Networks (CNN): HIgh Level\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>TL;DR</b> \n",
    "    <br>\n",
    "    <ul>\n",
    "        <li> A single unit in Fully Connected (FC) Layer identifies the presence/absence of a single feature spanning the <b>entire</b> input</li>\n",
    "        <li>A single \"kernel\" in a Convolutional Layer identifies the presence/absence of a single feature</li>\n",
    "        <ul>\n",
    "            <li>Whose size is a fraction of the entire input</li>\n",
    "            <li>At <b>each</b> sub-span of the input</li>\n",
    "        </ul>\n",
    "        \n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Example**\n",
    "- FC: is the input image the digit \"8\"\n",
    "- CNN: are there one or more small \"8\"'s in the input image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convolutional Neural Networks: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We have seen how the Fully Connected (FC) layer performs a template-matching on the layer's inputs.\n",
    "$$\n",
    "\\y_\\llp = a_\\llp ( \\W_\\llp \\y_{(\\ll-1)} + b )\n",
    "$$\n",
    "\n",
    "- Each element of $\\y_{(\\ll-1)}$ is independent\n",
    "    - there is no relationship between $\\y_{(\\ll-1), j}$ and $\\y_{(\\ll-1), j+1}$\n",
    "    - even though they are adjacent in the vector ordering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To see the lack of relationship:\n",
    "\n",
    "Let $\\text{perm}$ be a random ordering of the integers in the range $[1 \\ldots n]$.\n",
    "\n",
    "Then\n",
    "- $\\x[ \\text{perm} ]$ is a permutation of input $\\x$\n",
    "- $\\Theta[ \\text{perm} ]$ is the corresponding permutation of parametrs $\\Theta$.\n",
    "\n",
    "$$\n",
    "\\Theta^T \\cdot \\x = \\x[ \\text{perm} ] \\cdot \\Theta[ \\text{perm} ]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So a FC layer cannot take advantage of any explicit ordering among the input elements\n",
    "- timeseries of prices\n",
    "- adjacent pixels in an image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Another issue with an FC layer:\n",
    "- The \"template\" $\\W_\\llp$ matches the full length of the input $\\y_{(\\ll-1)}$ \n",
    "\n",
    "There are cases where we might want to discover a feature\n",
    "- whose length is less than $n$\n",
    "- that occurs *anywhere* in the input, rather than at a fixed location\n",
    "\n",
    "For example\n",
    "- a spike in a timeseries\n",
    "- the presence of an \"eye\" in an image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Both these questions motivate the notion of convolutional matching\n",
    "- small templates\n",
    "- that are slid over the entire input\n",
    "\n",
    "By sliding the pattern over the entire input\n",
    "- we can detect the existence of the feature somewhere in the input\n",
    "- we can localize its location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>CNN convolution</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/CNN_convolution_1.jpg\" width=900></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- We place $(3 \\times 3)$ Kernel (weight matrix) on the inputs ($\\y_{(0)}$, output of layer 0)\n",
    "- Performs dot product\n",
    "- Produces Layer 1 output ($\\y_{(1)}$) feature labelled $1$\n",
    "\n",
    "Slide the Kernel up and repeat:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>CNN convolution</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/CNN_convolution_2.jpg\" width=900></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The dot product *using the identical kernel weights* produces Layer 1 output ($\\y_{(1)}$) feature labelled $2$\n",
    "\n",
    "Repeat, centering the Kernel over each feature in $\\y_{(0)}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>CNN convolution</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/CNN_convolution_3.jpg\" width=900></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The output of a convolution is of similar size as the input\n",
    "- detects a specific feature *at each input location*\n",
    "\n",
    "So, for example, if there are\n",
    "- three spikes: will detect the \"is a spike\" feature in 3 locations\n",
    "- two eyes: will detect the \"is an eye\" feature in two locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The convolution operation is like\n",
    "- creating a small (size of template) FC layer\n",
    "- that is applied to each location\n",
    "\n",
    "So it \"fully connects\" *neighboring inputs* rather than the *entire input* and thus takes advantage\n",
    "of ordering present in the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The template is called a *kernel* or *filter*\n",
    "- The output of a convolution is called a *feature map*\n",
    "\n",
    "Pre-Deep Learning: manually specified filters have a rich history for image recognition.\n",
    "\n",
    "Let's see some in action to get a better intuition.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "<center>Fully Connected vs Convolution</center>\n",
    "<tr>\n",
    "<img src=\"images/CNN_vs_FC.jpg\" width=600\">\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Fully Connected Layer\n",
    "    - each of the $n_{(l-1)}$ units of layer $(l-1)$ connected to each of the $n_\\llp$ units of layer $\\llp$\n",
    "    = $n_{(l-1)} * n_\\llp$ weights total\n",
    "- Convolutional Layer with 1D convolution, filter size 3\n",
    "    - groups of $3$ units of layer $(l-1)$ connected to *individual* units of layer $\\llp$\n",
    "    - using the *same* 3 weights\n",
    "    - 3 weights total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So a Convolutional Layer can use *many fewer* weights/parameters than a Fully Connected Layer.\n",
    "\n",
    "As we will see, this enables us to create *many* separate convolutions in a single layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "_= cnnh.plot_convs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- A bright element in the output indicates a high, positive dot product\n",
    "- A dark element in the output indicates a low (or highly negative) dot product\n",
    "\n",
    "In our example, the kernel is $(3 \\times 3)$.\n",
    "\n",
    "The template match will be maximized when\n",
    "- high values in the input correspond to high values in the matching location of the template\n",
    "- low values in the input correspond to low values in the matching locations of the template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is a list of manually constructed kernels (templates) that have proven useful\n",
    "- [list of filter matrices](https://en.wikipedia.org/wiki/Kernel_(image_processing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- How do we construct a \"good\" kernel ?\n",
    "- How do we decide which one to use ?\n",
    "\n",
    "It all depends on the objective.\n",
    "\n",
    "Machine Learning to the rescue: let an ML algorithm \"learn\" the kernel that is best suited to the task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example, consider a two layer Sequential model\n",
    "- First layer is a convolution with kernel $\\kernel$\n",
    "- Second layer if a classifier with parameters $\\Theta$\n",
    "\n",
    "Let $\\loss$ be some loss function appropriate to classifcation, e.g, cross entropy.\n",
    "\n",
    "Then our ML Swiss Army Knife (Gradient Descent) solves for the loss-minimizing values of $\\Theta, \\kernel$\n",
    "$$\n",
    "\\Theta, \\kernel = \\argmin{\\Theta, \\kernel} \\loss\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Multiple kernels\n",
    "\n",
    "We have thus far seen a *single* kernel, applied to a $N=2$ dimensional input $\\y_{(\\ll-1)}$.\n",
    "\n",
    "The output $\\y_\\llp$ is an $N$ dimensional feature map that identifies the presence/absence\n",
    "of a feature at each element of $\\y_{(\\ll-1)}$.\n",
    "\n",
    "Why not use *multiple* kernels to identify *multiple* features ?\n",
    "- Let Convolutional Layer $\\ll$ have $n_{\\llp,1}$ kernels\n",
    "- The output is $n_{\\llp,1}$ feature maps, one per kernel, identifying the presence/absence of one feature each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is similar in concept to a Fully Connected layer\n",
    "- Let FC layer $\\ll$ have $n_\\llp$ units/neurons\n",
    "- The output is a vector of $n_\\llp$ features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let $(n_{(\\ll-1),1} \\times n_{(\\ll-1),2})$ denote the shape of $\\y_{(\\ll-1)}$.\n",
    "\n",
    "If Convolutional Layer $\\ll$ has $n_{\\llp,1}$ kernels\n",
    "- the output shape is $(n_{\\llp,1} \\times n_{(\\ll-1),1} \\times n_{(\\ll-1),2})$\n",
    "\n",
    "That is, the input is replicated $n_\\llp$ times, one per kernel of layer $\\ll$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "       <center>CNN convolution</center>\n",
    "    <tr>\n",
    "        <td><img src=\"images/CNN_feature_map.jpg\" width=600></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It also means that the matrix $\\kernel_\\llp$ representing the kernels at layer $\\ll$\n",
    "has the same number of dimensions as the output\n",
    "- the first dimension is the number of kernels\n",
    "- the rest of the dimensions are the size of each kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Higher dimensional input ($N > 2$)\n",
    "\n",
    "After applying $n_{\\llp,1}$ kernels to $N=2$ dimensional input $\\y_{(\\ll-1)}$\n",
    "of shape\n",
    "$(n_{(\\ll-1),1} \\times n_{(\\ll-1),2})$ \n",
    "we get a three dimensional output of shape $(n_{\\llp,1} \\times n_{(\\ll-1),1} \\times n_{(\\ll-1),2})$.\n",
    "\n",
    "What happens when input $\\y_{(\\ll-1)}$ is $N=3$ dimensional \n",
    "of shape\n",
    "$(n_{(\\ll-1),1} \\times n_{(\\ll-1),2} \\times _{(\\ll-1),3})$ ?\n",
    "\n",
    "- this can easily occur in layer $(\\ll-1)$ is a Convolutional Layer with $n_{(\\ll-1),1}$ kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If $\\y_{(\\ll-1)}$ is the output of a Convolutional Layer\n",
    "- $\\y_{(\\ll-1)}$ has $n_{(\\ll-1),1}$ features over a space of shape $(n_{(\\ll-1),2} \\times _{(\\ll-1),3})$\n",
    "\n",
    "Convolutional Layer $\\ll$ \n",
    "- combines  all $n_{(\\ll-1),1}$ input features at a single \"location\" \n",
    "- into a new synthetic *scalar* feature at the same location in output $\\y_\\llp$\n",
    "\n",
    "This means the output $\\y_\\llp$ is of shape $(n_{\\llp,1} \\times n_{(\\ll-1),2} \\times _{(\\ll-1),3})$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This implies that *each kernel* of layer $\\ll$ is of dimension $N_{(\\ll-1)}$\n",
    "\n",
    "For example: if layer $(\\ll -1)$ has $N_{(\\ll -1)} = 3$ dimensions\n",
    "- each kernel of layer $\\ll$ is has 3 dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Kernel shape\n",
    "\n",
    "We see that a Convolutional Layer $\\ll$ transforms\n",
    "- an input of dimension $(n_{(\\ll-1),1} \\times n_{(\\ll-1),2} \\times n_{(\\ll-1),3})$\n",
    "- into output with dimension $(n_{\\llp,1} \\times n_{(\\ll-1),2} \\times n_{(\\ll-1),3})$\n",
    "\n",
    "If each kernel of layer $\\ll$ has shape $(k_1 \\times k_2)$ then the kernel matrix $\\kernel_\\llp$ for layer $\\ll$\n",
    "- has shape $(n_{\\llp,1} \\times k_1 \\times k_2)$\n",
    "- one kernel per output synthetic feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Simple case: 1d convolution\n",
    "\n",
    "We have thus far illustrated Convolution with input layer $0$ having $\\x^\\ip$ of dimension $N_{(0)} \\in \\{2,3 \\}$.\n",
    "\n",
    "We can generalize the logic to tensors of dimension $N > 3$.\n",
    "\n",
    "But we also have the simplest case of $N=1$.\n",
    "- can consider the one dimensional $\\x^\\ip$ has being two dimensional with leading dimension $1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One dimensional convolution is quite common\n",
    "- timeseries of prices\n",
    "- sequence of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Show order\n",
    "\n",
    "Consider a time series of prices of length 5\n",
    "- a positive spike at elements 1 and 3\n",
    "- a FC has no order\n",
    "    - can't distinguish between $[+,-+]$ and $[+,+,-]$\n",
    "    - but a 1D convolution with kernel size 3 can\n",
    "\n",
    "Consider a sequence of words\n",
    "- an FC cannot distinguish $[\"not\", \"like\", \"ML\"]$ from $[\"ML\", \"not\", \"like\"]$\n",
    "    - but a 1D convolution with kernel size 3 can"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So Convolutional Layers can impose a partial ordering (within range of kernel) where FC Layers cannot.\n",
    "\n",
    "This doesn't completely address the issue of inputs that are sequences as the \"field\"\n",
    "of ordering is only within (a small) kernel.\n",
    "\n",
    "We will learn to deal with sequences when we study Recurrent Neural Networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Terminology\n",
    "\n",
    "- a *kernel* or *filter* is a pattern\n",
    "    - slide over each element of the input\n",
    "- a *feature map* or *activation map* is the output of applying a single kernel to the input\n",
    "    - similar shape to the input (will discuss padding)\n",
    "    - identifies the presence/absence of a feature *at each* location of the input space\n",
    "Geron, page 447\n",
    "- filters, kernels\n",
    "- activation map, feature maps\n",
    "    - output of a layer\n",
    "    - latent representation\n",
    "- 3D volume\n",
    "    - width (horizontal coordinate), height (vertical), channel (depth)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Border control: Padding\n",
    "\n",
    "Assuming full padding, a layer $\\ll$ Convolutional Layer with $n_{\\llp,1}$ kernels will have output $\\y_\\llp$ dimension\n",
    "- $(n_{\\llp,1} \\times n_{(\\ll-1),2} \\times n_{(\\ll-1),3})$\n",
    "    - $n_{\\llp,1}$ features\n",
    "    - over a spatial map of $(n_{(\\ll-1),2} \\times n_{(\\ll-1),3})$\n",
    "    \n",
    "That is, the number of features changes but the spatial dimension is similar to $\\y_{(\\ll-1)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This that the \"dot product\" notation $\\kernel \\cdot \\y_{(\\ll-1)}$\n",
    "is extended over higher dimensions\n",
    "- that is: flatten both $\\kernel$ and $\\y_{(\\ll-1)}$$ and apply the one dimensional dot product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Technical point\n",
    "- math definition of convolution\n",
    "    - dot product of input and *reversed* filter ?\n",
    "    - we are doing [cross correlation](https://en.wikipedia.org/wiki/Convolution_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Size of output\n",
    "\n",
    "- input $W_i \\times H_i \\times D_i$\n",
    "- \n",
    "- $N$: input size $N \\times N$\n",
    "- $F$: filter size $F \\times F$\n",
    "- $S$: stride\n",
    "- $P$: padding\n",
    "\n",
    "No padding\n",
    "- output size $( (W_i -F)/S +1 ) \\times ( (H_i - F)/S +1 ) \\times D_o$$\n",
    "\n",
    "Padding\n",
    "- output size $( (W_i -F +2P)/S +1 ) \\times ( (H_i - F +2P)/S +1 ) \\times D_o$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Number of parameters\n",
    "\n",
    "The real power of convolution comes from using the *same* filter against all locations of the input.\n",
    "As a result, the number of parameters is quite small (compared to a separate set of paramters per each input).\n",
    "\n",
    "- Dimension of a single filter $F \\times F \\times D_i$\n",
    "- $D_o$: number of output filters\n",
    "- total parameters: $F * F * D_i * D_o$\n",
    "\n",
    "Remember: there is a depth to the input and the filter applies to the entire input depth\n",
    "- size of a filter $F *F * D_i$\n",
    "- number of filters: $D_o$, one per output channel\n",
    "- total: $F * F * D_i * D_o$\n",
    "\n",
    "If we were to have a separate filter for each input location, the number of parameters would increase\n",
    "by a factor of $W_i * H_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pooling\n",
    "\n",
    "After sliding the filter over the input, the output of a single feature map\n",
    "- indicates whether at locations\n",
    "\n",
    "\n",
    "Sliding filter, with padding, gives intermediate representation of same size.\n",
    "\n",
    "Still big.\n",
    "\n",
    "Can \"down sample\" to get smaller image that gives you a wider, but fuzzier view.\n",
    "- **does this depend on stride of pooling ?**\n",
    "\n",
    "From \"feature  yes/no at particular coordinate of input space\" to \"feature yes/no in wider region\"\n",
    "\n",
    "Mechanically: pooling is just like the convolution operation\n",
    "- replace multiplication of volume (feature map) slice and filter by pooling operation\n",
    "- Spatial invariance \n",
    "    - no longer care about coordinate -- just whether feature present\n",
    "    - Picasso cat\n",
    "        - 2 eyes, not necessarily horizontal !\n",
    "        \n",
    "Pooling operations\n",
    "-Average pooling\n",
    "    - average over the volume\n",
    "    \n",
    "- Max pooling\n",
    "    - Max over the volume\n",
    "- K-Max pooling\n",
    "    - select the K largest elements of the volume\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "- Down samples input to smaller size\n",
    "    - *exact* location not important\n",
    "        - in general area\n",
    "    - smaller activation volume\n",
    "        - faster\n",
    "    - fewer number of paramaters in successor layer (especially for FC layers, not so much for Conv)\n",
    "    - destructive, looses inforomation\n",
    "        - blocky artifacts\n",
    "- How does back prop work on max pooling ??\n",
    "    - Zeiler and Fergus record location of max -- is this used ?\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Strides\n",
    "\n",
    "- Down samples\n",
    "    - see Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Translation invariance\n",
    "\n",
    "When we slide the filter over the layer's inputs, it will react to the pattern, wherever it occurs\n",
    "and however many times it occurs.\n",
    "\n",
    "When we use pooling, we may lose location and frequency of occurence (e.g., max).\n",
    "This is usually not an impediment but can lead to some unintuitive results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "Image(\"images/Picasso_Cat.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convolution as matrix multiplication\n",
    "[A guide to convolutional arithmetic for deep learning](https://arxiv.org/pdf/1603.07285.pdf)\n",
    "\n",
    "Geron equation 13-1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can turn convolution into matrix multiplication.\n",
    "\n",
    "For simplicity, we will show this for a single channel, using a $(3 \\times 3)$ kernel on a $(4 \\times 4 \\times 1)$ input volume.\n",
    "\n",
    "Basically: we flatten out both the kernel matrix $W$ \n",
    "\n",
    "$$\n",
    "W = \\begin{pmatrix}\n",
    "w_{0,0} & w_{0,1} & w_{0,2} \\\\\n",
    "w_{1,0} & w_{1,1} & w_{1,2} \\\\\n",
    "w_{2,0} & w_{2,1} & w_{2,2}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "and the input volume matrix.\n",
    "\n",
    "Since the input volume is $(16 \\times 1)$, we will left multiply by a matrix with number of rows equal to the output volume, and $16$ columns.\n",
    "\n",
    "For simplicity, we do this without padding, so the output volume is $(2 \\times 2)$ which flattened is $(4 \\times 1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$C = \\begin{pmatrix}\n",
    "    w_{0,0} & w_{0,1} & w_{0,2} & 0 & w_{1,0} & w_{1,1} & w_{1,2} & 0 & w_{2,0} & w_{2,1} & w_{2,2} & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "    0       & w_{0,0} & w_{0,1} & w_{0,2} & 0 & w_{1,0} & w_{1,1} & w_{1,2} & 0 & w_{2,0} & w_{2,1} & w_{2,2} & 0 & 0 & 0 & 0 \\\\\n",
    "    0       & 0       & 0       & 0       & w_{0,0} & w_{0,1} & w_{0,2} & 0 & w_{1,0} & w_{1,1} & w_{1,2} & 0 & w_{2,0} & w_{2,1} & w_{2,2} & 0 \\\\\n",
    "    0 & 0       & 0       & 0       & 0       & w_{0,0} & w_{0,1} & w_{0,2} & 0 & w_{1,0} & w_{1,1} & w_{1,2} & 0 & w_{2,0} & w_{2,1} & w_{2,2} \n",
    "  \\end{pmatrix}\n",
    "  $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Once you understand that the convolution result is obtained as $C X'_{l}$ (where $X'_{l}$ is the flattened inputs to layer $l$), you can imagine an inverse of $C$ to go from the convolution result\n",
    "backwards to $X'_{l}$.\n",
    "\n",
    "That is, we can trace backwards from each activation in a feature map to the inputs that went into its\n",
    "computation.\n",
    "\n",
    "This will enable us to do back propagation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Inverting convolution\n",
    "\n",
    "For a variety of reasons, it will prove useful to invert the convolution operation.\n",
    "\n",
    "For example\n",
    "- if we view convolution as down-sampling, there will be cases where we want to restore the original volume by up-sampling\n",
    "- we want to know which elements of the input volume contribute to a particular element of the output volume\n",
    "    - need to back propagate the gradient\n",
    "- we may want to know which elements of the input volume contribute most to an element of the output volume (perhaps an output volume several layers removed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will discuss these in the context of understanding what a layer of a CNN is \"looking for\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multiple layers\n",
    "\n",
    "Multiple transformations\n",
    "\n",
    "- Increasing Receptive field\n",
    "    - math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How to choose ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How many filters to use (what is the correct  number of channels ?)\n",
    " [Bag of Tricks for Image Classification with CNNs](https://arxiv.org/abs/1812.01187)\n",
    " \n",
    "Suppose the kernel size for a CNN layer is $(W \\times H \\times D)$ (thus operating on an input whose channel depth is $D$).\n",
    "\n",
    "Then each convolution dot product is a function of $N = (W*H*D)$ inputs.\n",
    "Having more than $N$ output channels is the opposite of compressing the input: we are generating more\n",
    "values than in the input.\n",
    "\n",
    "So we can argue for $N$ as an  upper bound.\n",
    "\n",
    "This arguement is mitigated somewhat by the \"lottery ticket\" argument: having extra neurons, even if many are eventually \"dead\" (unused, and hence can be pruned), facilitates training\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What Kernel size to use ?\n",
    "\n",
    "Early CNN's tended to use larger kernels, but current belief argues for a width and height of $3 \\times 3$.\n",
    "One can achieve the effect of a larger receptive field (what a bigger kernel achieves) through multiple\n",
    "CNN layers.\n",
    "\n",
    "There is some evidence that using deeper networks (more CNN layers) is more effective than a larger kernel size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Receptive field size\n",
    "\n",
    "The *receptive field* of an activation map refers to the Layer 0 (input) features that affect features in the map.\n",
    "\n",
    "As you go one layer deeper in the NN, the receptive field width and height increase by (2 * *stride*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>CNN receptive field</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/CNN_Receptive_field.jpg\" width=600></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Each grid in Layer 1 refers to the *same* features in Layer 0\n",
    "- The layer 2 feature labelled $i$ is a function of the Layer 1 features labelled $i$\n",
    "- By completing the $(3 \\times 3)$ grid in Layer 2:\n",
    "    - all $(5 \\times 5)$ layer 0 features are touched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "|  Layer  | Receptive field |\n",
    "|-- |-- |\n",
    "1 | $(3 \\times 3)$\n",
    "1 | $(5 \\times 5)$\n",
    "1 | $(7 \\times 7)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

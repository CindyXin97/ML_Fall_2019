{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DNN_TensorFlow_example.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"0IDpTj3R2War","colab_type":"text"},"source":["# Derived from Geron 11_deep_learning.ipynb"]},{"cell_type":"markdown","metadata":{"id":"n0VTrSvDJlis","colab_type":"text"},"source":["We will provide a quick introduction into programming with TensorFlow.\n","\n","We revist our old friend, MNIST digit classification and provide two solutions\n","- the first using \"raw\", low-level TensorFlow\n","- the second using the high-level Keras API"]},{"cell_type":"code","metadata":{"id":"PEkMfar38_Q7","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","\n","import numpy as np\n","import os\n","\n","import pdb"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SyT0csrdV2X7","colab_type":"text"},"source":["# Raw TensorFlow"]},{"cell_type":"markdown","metadata":{"id":"nYdRJiVS2k7c","colab_type":"text"},"source":["# TensorFlow.layers\n","\n","We will build an MNIST classifier using TensorFlow.layers"]},{"cell_type":"markdown","metadata":{"id":"T0RoR15U3N2_","colab_type":"text"},"source":["## Get the MNIST dataset\n","- data pre-split into training and test sets\n","  - flatten the images from 2 dimensional to 1 dimensional (makes it easier to feed into first layer)\n","  - create validation set from part of training\n","- \"normalize\" the inputs: change pixel range from [0,255] to [0,1]"]},{"cell_type":"code","metadata":{"id":"g3Cdg32u3Lon","colab_type":"code","colab":{}},"source":["(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n","\n","# Determine \n","# - the dimensions of the input by examining the first training example\n","# - the dimensions of the output (number of classes) by examinimg the targets\n","input_size = np.prod(X_train[0].shape)\n","output_size = np.unique(y_train).shape[0]\n","\n","# input image dimensions\n","img_rows, img_cols = X_train[0].shape[0:2]\n","\n","valid_size = X_train.shape[0] // 10\n","\n","# Flatten the data to one dimension and normalize to range [0,1]\n","X_train = X_train.astype(np.float32).reshape(-1, input_size) / 255.0\n","X_test = X_test.astype(np.float32).reshape(-1, input_size) / 255.0\n","y_train = y_train.astype(np.int32)\n","y_test = y_test.astype(np.int32)\n","X_valid, X_train = X_train[:valid_size], X_train[valid_size:]\n","y_valid, y_train = y_train[:valid_size], y_train[valid_size:]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"US5Qs7m7AA66","colab_type":"code","outputId":"0fcc6213-9497-4805-b321-fcb857f15970","executionInfo":{"status":"ok","timestamp":1571275135602,"user_tz":240,"elapsed":1715,"user":{"displayName":"K Perry","photoUrl":"","userId":"06663495321778630978"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["X_train.shape"],"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(54000, 784)"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"IV00RzrR23aF","colab_type":"code","colab":{}},"source":["# Placeholders for input X, target y\n","#  The first dimension (None) is for the batch size\n","\n","X = tf.placeholder(tf.float32, shape=(None, input_size), name=\"X\")\n","y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EL4y6kMp825O","colab_type":"text"},"source":["## Create function to return mini-batches"]},{"cell_type":"code","metadata":{"id":"tKWxF7CF87TM","colab_type":"code","colab":{}},"source":["def next_batch(X, y, batch_size, shuffle=True):\n","  \"\"\"\n","  Generator to return batches from X and y\n","  \n","  Parameters\n","  ----------\n","  X: ndarray\n","  y: ndarray.  The first dimension of X and y must be the same\n","  batch_size: Int.  The size of the slice (of X and y) to return in each batch\n","  shutffle: Boolean.  Sample X, y in random order if True\n","  \n","  Yields\n","  ------\n","  X_batch, y_batch: a 2-tuple of ndarrays, \n","  - where X_batch is a slice (of size at most batch_size) of X\n","  - where y_batch is a slice of y (same first dimension as X_batch)\n","  \n","  If first dimension of X is not evenly divisible by batch size, the final batch will \n","  be of size smaller than batch_size\n","  \"\"\"\n","  \n","  # Randomize the indices\n","  if shuffle:\n","    idx = np.random.permutation(len(X))\n","  else:\n","    idx = np.arange( len(X) )\n","\n","  # Return a batch of size (at most) batch_size, \n","  # starting at idx[next_start] \n","  next_start = 0\n","\n","  n_batches = len(X) // batch_size\n","  \n","  while next_start < len(X):\n","    # Get a batch of indices from idx, starting a idx[next_start] and ending at idx[next_end]\n","    next_end   = min(next_start + batch_size, len(X))\n","    X_batch, y_batch = X[ idx[next_start:next_end] ], y[ idx[next_start:next_end] ]\n","\n","    # Advance next_start to start of next batch\n","    next_start = next_start + batch_size\n","\n","    # Return a batch\n","    yield X_batch, y_batch\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q7lOsZRd_py6","colab_type":"text"},"source":["## Build the computation graph"]},{"cell_type":"code","metadata":{"id":"W_RJw1dCT_WO","colab_type":"code","colab":{}},"source":["# to make this notebook's output stable across runs\n","def reset_graph(seed=42):\n","    tf.reset_default_graph()\n","    tf.set_random_seed(seed)\n","    np.random.seed(seed)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"K6f0gWZm_sTQ","colab_type":"code","colab":{}},"source":["reset_graph()\n","\n","(n_hidden_1, n_hidden_2) = (100, 30)\n","\n","# Placeholders for input X, target y\n","#  The first dimension (None) is for the batch size\n","X = tf.placeholder(tf.float32, shape=(None, input_size), name=\"X\")\n","y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n","\n","with tf.name_scope(\"dnn\"):\n","    hidden1 = tf.layers.dense(X, n_hidden_1, activation=\"relu\", name=\"hidden1\")\n","    hidden2 = tf.layers.dense(hidden1, n_hidden_2, activation=\"relu\", name=\"hidden2\")\n","    logits = tf.layers.dense(hidden2, output_size, name=\"outputs_\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XcKWUh7bAzTw","colab_type":"text"},"source":["## Create a loss node\n","- Use cross entropy as loss \n","  - we are comparing the probability vector computed by the graph (logits) with the target probability vector (y)\n","  \n","Ordinarily we would need to\n","- convert the scores (logits) vector to a probability vector  by a *softmax* activation on the \"outputs\" layer\n","- convert the target to a one-hot vector (length equal to number of target classes, which is also length of probability vector)\n","- compare the two vectors with cross_entropy\n","\n","TensorFlow provides a very convenient method `sparse_softmax_cross_entropy_with_logits` that does all the work for us !\n","- applies `softmax` to the scores (logits)\n","- converts integer targets (in range [0, number of classes]) into one-hot vectors (with length equal to number of classes)\n","- does the cross entropy calculation"]},{"cell_type":"code","metadata":{"id":"dV7TYru5A-FX","colab_type":"code","colab":{}},"source":["with tf.name_scope(\"loss\"):\n","  # xentropy is a tensor whose first dimension is the batch size\n","  xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n","  \n","  # Find the loss across the examples in the batch by summing individual example losses\n","  loss = tf.reduce_mean(xentropy, name=\"loss\")\n","  \n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RXUDMB6NERjz","colab_type":"text"},"source":["## Create a node to compute accuracy \n","-  for each example, compares the element in the logit vector with the highest score (i.e., index of our prediction) to the target\n","- sums up the number of examples with matching max logit and target"]},{"cell_type":"code","metadata":{"id":"p96JUARmEQhz","colab_type":"code","colab":{}},"source":["with tf.name_scope(\"eval\"):\n","  correct = tf.nn.in_top_k(logits, y, 1)\n","  accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s4fn7An1D2Px","colab_type":"text"},"source":["## Create the training operations\n","- Training operation is an optimizer step that minimizes the loss"]},{"cell_type":"code","metadata":{"id":"hEs3eok8D8Vz","colab_type":"code","colab":{}},"source":["learning_rate = 0.01\n","\n","with tf.name_scope(\"train\"):\n","  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n","  training_op = optimizer.minimize(loss)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rxYzMnDWG9d5","colab_type":"text"},"source":["## Create an initialization node to initialize global variables (i.e., the weights that the optimizer will solve for)\n"]},{"cell_type":"code","metadata":{"id":"AcG0YcU2HEDm","colab_type":"code","colab":{}},"source":["init = tf.global_variables_initializer()\n","saver = tf.train.Saver()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I9TW48GdEwlZ","colab_type":"text"},"source":["## Run  the training loop\n","- Run for multiple \"epochs\"; an epoch is an entire pass through the training data set\n","- For each epoch, divide the training set into mini-batches\n","  - For each mini-batch\n","    - run the \"training operation\" (i.e, the optimizer)\n","    - every few epochs\n","      - compute the accuracy (by evaluating the graph node that computes accuracy) on the training and validation set\n","      \n","In general, we usually continue training\n","- as long as the validation loss continues to decrease across epochs\n","- as long as the validation loss is greater than the training loss\n","  - if the training loss is much lower than the validation (out of sample) loss, we may be overfitting to the training data\n","  - **Note** that we have stated these conditions in terms of decreasing validation loss, rather than increasing validation accuracy\n","    - **Question**: *Why should we prefer \"loss\" to \"accuracy\" ?*\n"," "]},{"cell_type":"code","metadata":{"id":"UwZ9NvFCEzcJ","colab_type":"code","colab":{}},"source":["n_epochs = 20\n","batch_size = 50\n","\n","modelName = \"mnist_first\"\n","\n","save_path = os.path.join(\".\", modelName + \".ckpt\") "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"t5aYMTwk_fry","colab_type":"code","outputId":"573cbfb0-e511-450a-de99-c8a5667efefd","executionInfo":{"status":"ok","timestamp":1571273874083,"user_tz":240,"elapsed":35998,"user":{"displayName":"K Perry","photoUrl":"","userId":"06663495321778630978"}},"colab":{"base_uri":"https://localhost:8080/","height":89}},"source":["print(\"Training for {e:d} epochs\".format(e=n_epochs))\n","\n","# Create a session and evaluate the nodes within it\n","with tf.Session() as sess:\n","  # Run the initialization step\n","  init.run()\n","  \n","  # This is our main training loop\n","  # - run for multiple epochs\n","  # - in each epoch, process the entire training data in mini-batches\n","  for epoch in range(n_epochs):\n","    # Process each of the mini-batches, evaluating the training operation on each\n","    for X_batch, y_batch in next_batch(X_train, y_train, batch_size, shuffle=True):\n","      sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n","        \n","    # Measure the training and validation accuracy every few epochs \n","    if epoch % 5 == 0:\n","        acc_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n","        acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n","        print(\"Epoch {e:d} training batch accuracy {ta:3.2f}%, validation set accuracy {va:3.2f}%:\".format(e=epoch, ta=100*acc_batch, va=100*acc_valid) )\n","\n","  # Save the session so we can pick up again      \n","  save_path = saver.save(sess, save_path)\n","  \n","  print(\"Trained\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Training for 20 epochs\n","Epoch 0 training batch accuracy 92.00%, validation set accuracy 88.78%:\n","Epoch 5 training batch accuracy 86.00%, validation set accuracy 94.50%:\n","Epoch 10 training batch accuracy 96.00%, validation set accuracy 96.03%:\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WMp3oR3W04MB","colab_type":"text"},"source":["Now that the model is trained (and saved) we can feed in test data in order to perform predictions\n","\n","Note that:\n","- The graph must always be evaluated in a session\n","- A new session is completely uninitialized\n","  - the trained weights are *not* available\n","- We can restore the state of a previous session, in order to obtain access to the trained model"]},{"cell_type":"code","metadata":{"id":"wecpagmYx8RU","colab_type":"code","colab":{}},"source":["with tf.Session() as sess:\n","  # Restore the model, do NOT re-initialize the variables with the \"init\" node\n","  saver.restore(sess, save_path)\n","  \n","  # We can now evaluate any of the model's nodes, using the trained weights\n","  # Perform prediction using the test set\n","  # Recall: the logits for each example is a vector of length, number of classes\n","  # To convert one vector to a prediction: find the index of the largest logit\n","  logits_test = logits.eval(feed_dict={X: X_test})\n","  print(\"Test logits shape: \", logits_test.shape)\n","  predictions_test = np.argmax(logits_test, axis=1)\n","  \n","  # Show some of the predictins\n","  num_to_show = 10\n","  print(\"Test predictions: \\t\",  predictions_test[:num_to_show])\n","  \n","  print(\"Test correct ?:\\t \",    (predictions_test == y_test)[:num_to_show])\n","  \n","  # What is the overall accuracy ?\n","  acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n","  print(\"Test accuracy {a:3.2f}\".format(a=100*acc_test))\n","  \n","  \n","\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BrvbOmCvMcgi","colab_type":"text"},"source":["# Keras version\n","\n","That was very instructive (hopefully) but also a lot of detailed work.\n","\n","It's worthwhile studying the TensorFlow.layers to get a deeper understanding of \n","- computation graph\n","  -definition\n","  - initialization\n","  - evaluation\n","- loss functions:\n","  - computed per example and summed\n","- the training loop\n","\n","Over the years, many people have created higher level abstractions (e.g., `tf.layers.dense` is an abstraction that saves you the trouble of multiplying inputs by weights, adding a bias, and applying an activation) to both simplify and reduce repeated code patterns.\n","\n","The Keras API is a very high level abstraction (that looks similar to `sklearn` in some regards) that simplifies things a great deal, and will be tightly integrated into TensorFlow 2.0\n","\n","Let's re-implement this classification problem in Keras"]},{"cell_type":"markdown","metadata":{"id":"hPeAgHXoKCex","colab_type":"text"},"source":["## Boiler plate\n","\n","Here are some of our standard imports.\n","\n","Note that - `keras` and - `tensorflow.keras` are two very similar but **distinct** modules !  \n","- `keras` is a  [project](https://keras.io/) that is separate and distinct from TensorFlow\n","  - It is an API for Neural Network programming, not a library\n","  - The API can be implemented for many different compute engines.  TensorFlow is just one engine\n","  - The Keras project supplies a TensorFlow engine which is **not identical** to Google's tTensorFlow implementation\n","  \n","- `tensorflow.keras` is Google's implementation (and extension) of the Keras API\n","\n","For the most part they are similar, but you can create difficulty if you mix and match.\n","We will deal exclusively with `tensorflow.keras`, as will be reflected in our import statements.\n"]},{"cell_type":"code","metadata":{"id":"fuIxxo8DzrB4","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","from tensorflow.keras.models import Model, Sequential\n","from tensorflow.keras import layers\n","\n","from tensorflow.keras.utils import plot_model\n","import IPython\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JUsOYDS8MG4q","colab_type":"text"},"source":["## Build the compuation graph in Keras"]},{"cell_type":"code","metadata":{"id":"hpY4y5n6z-VA","colab_type":"code","colab":{}},"source":["mnist_model = Sequential([ layers.Dense(n_hidden_1, activation=tf.nn.relu,    name=\"hidden1\", input_shape=(input_size,)),\n","                           layers.Dense(n_hidden_2, activation=tf.nn.relu,    name=\"hidden2\"),\n","                           layers.Dense(output_size,activation=tf.nn.softmax, name=\"outputs\")\n","                         ]\n","                        )\n","    \n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vls6hFpTMM_2","colab_type":"text"},"source":["That was easy !\n","\n","We used the same conceptual layers as in the TensorFlow.layers implementation and passed them as a list to the `Sequential` model.\n","Note, however, that the \"layers\" now come from tf.keras rather than TensorFlow tf.\n","\n","The `Sequential` model will take the input, feed it to the first layer, and pass the output of layer $i$ to the input of layer $i+1$.\n","\n","Some things to point out\n","- when you use the `Sequential` model, you don't supply an explicit `Input` layer (placeholders in TensorFlow.layers)\n","  - instead: the first (and only the first) layer requires the `input_shape` argument to describe the shape of the input\n","- Unlike in the TensorFlow.layers code, the  final layer (`outputs`) has an `softmax` activation\n","  - In TensorFlow.layers, the loss function (`sparse_softmax_cross_entropy_with_logits`) performs its own `softmax`\n","    - we couldn't find a similar loss function in Keras, so we perform the `softmax` ourself."]},{"cell_type":"markdown","metadata":{"id":"CkJoK62xN72s","colab_type":"text"},"source":["### Creating a loss node  and traiining operation in Keras \n","\n","The `mnist_model` specifies the layers of the model, but doesn't actually build the computation graphs.\n","For that, we need to \"compile\" the model.\n","\n","The compile step is also where we specify \n","- the loss function\n","- the optimizer step\n","- other \"metrics\" (values to measure) to compute in the training loop\n","\n","Below \n","- we will use `sparse_categorical_crossentropy` as the loss (`sparse` because our labels are not one-hot encoded).\n","- `adam` as our optimizer (could have easily chosen `sgd`in order to be more similar to the TensorFlow.layers code)\n","- measure training accuracy (`acc`) "]},{"cell_type":"code","metadata":{"id":"v6jzbbOi73iL","colab_type":"code","colab":{}},"source":["metrics = [ \"acc\" ]\n","mnist_model.compile(optimizer='adam',\n","                    loss='sparse_categorical_crossentropy',\n","                    metrics=metrics)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OCuhn38TPyIU","colab_type":"text"},"source":["## History and callbacks\n","\n","Strictly speaking, the next few cells are not absolutely necessary: they go far beyond what our TensorFlow.layers program accomplishes\n","- call backs\n","  - these are functions that are called automatically in the training loop\n","    - `EarlyStopping`is a call back that will terminate the training loop when it is no longer productive to continue (e.g., when validation loss levels off)\n","    - `ModelCheckpoint` is a call back that will create intermediate snapshots of our model (including the parameters/weights it has learned)\n","      - We will create a checkpoint whenever accuracy improves.  So if further training reduces accuracy, we can restore back to the \"best\" parameter values.\n","      - This means we can re-start the model and continue to train without losing the \"best\" values.\n","      - In the TensorFlow.layers code, we only created a single checkpoint at the end of training\n","  "]},{"cell_type":"code","metadata":{"id":"4xA_buEbBORy","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","\n","def plot_training(history, metrics=[]):\n","    \"\"\"\n","    Plot training and validation statistics\n","    - accuracy vs epoch number\n","    - loss     vs epoch number\n","\n","    From https://www.learnopencv.com/keras-tutorial-fine-tuning-using-pre-trained-models/\n","    \"\"\"  \n","\n","    # Loss\n","    loss = history.history['loss']\n","\n","    epochs = range(len(loss))\n","\n","    plt.plot(epochs, loss, 'b', label='Training loss')\n","    plt.title('Training loss')\n","    plt.legend()\n","\n","    plt.figure()\n","\n","    for metric in metrics:\n","      metric_value = history.history[metric]\n","      plt.plot(epochs, metric_value, 'b', label=\"Training \" + metric)\n","      plt.title('Training  accuracy')\n","      plt.legend()\n","\n","    plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Odi6HQsH_yPs","colab_type":"code","colab":{}},"source":["# Load the TensorBoard notebook extension\n","%load_ext tensorboard\n","\n","import datetime\n","\n","import os\n","\n","logs_dir=\"logs/fit/\"\n","os.makedirs( \".\", exist_ok=True)\n","\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","log_dir= os.path.join(logs_dir, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") )\n","\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5m68M2RwRPJt","colab_type":"text"},"source":["### Create call backs\n","- Early Stopping\n","- Model Checkpoint"]},{"cell_type":"code","metadata":{"id":"ofX0fwl1_p-P","colab_type":"code","colab":{}},"source":["es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=.00005, patience=2, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n","\n","callbacks = [ es_callback,\n","              ModelCheckpoint(filepath=modelName + \".ckpt\", monitor='acc', save_best_only=True)\n","          ]\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c1rXcjm9RbWl","colab_type":"text"},"source":["## Run the training loop in Keras\n","\n","Now that the model is compiled, we can run `fit` on our training (and validation) data sets/\n","\n","This is very much  like `sklearn`.\n","\n","Note\n","- We don't have to construct our own training loop\n","- We don't have to create code to deliver mini-batches\n","- We don't have to insert code to display metrics (like accuracy)\n","- We don't have to run for the full set of epochs, because of Early Stopping\n","\n","See how much simpler this step is compared to TensorFlow.layers."]},{"cell_type":"code","metadata":{"id":"jgFahEm19KoJ","colab_type":"code","colab":{}},"source":["history = mnist_model.fit(X_train, y_train, epochs=n_epochs, batch_size=batch_size, validation_data=(X_valid, y_valid), shuffle=True, callbacks=callbacks)\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LhfwC058SNMq","colab_type":"text"},"source":["### Compute the accuracy on the test set"]},{"cell_type":"code","metadata":{"id":"VnunhN-fC29O","colab_type":"code","colab":{}},"source":["test_loss, test_accuracy = mnist_model.evaluate(X_test, y_test)\n","print(\"Test dataset: loss={tl:5.4f}, accuracy={ta:5.4f}\".format(tl=test_loss, ta=test_accuracy))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kL10YmhkSa45","colab_type":"text"},"source":["## See the training history\n","\n","The `fit` method returns a `history` object, which contains a time-series (across the epochs) of each metric.\n","\n","You automatically get a `loss` metric so you can see how quickly your training loss decreases.\n","\n","In the compile step, you can add other metric (like accuracy, both for training and validation).\n","\n","Because these metrics are time series, we can visualize them."]},{"cell_type":"code","metadata":{"id":"V7-1nlvrDJj4","colab_type":"code","colab":{}},"source":["history.history.keys()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_3M6OWchC_cR","colab_type":"code","colab":{}},"source":["plot_training(history, metrics)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MGIur_yGUc-K","colab_type":"text"},"source":["## Use the model for prediction\n","\n","Just as with `sklearn`, once we have fit the model, we can use the `predict` method to map inputs to predictions.\n","- Remember: the `outputs` layer has 10 elements, one per output class (so it is one-hot encoded)"]},{"cell_type":"code","metadata":{"id":"lGvVOjIGUgvc","colab_type":"code","colab":{}},"source":["predictions = mnist_model.predict(X_test)\n","predictions.shape"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2V6fW1zO3hkj","colab_type":"text"},"source":["## Examine the model\n","\n","Observe the number of parameters (weights) that the model requires.  Is it larger than you thought ?"]},{"cell_type":"code","metadata":{"id":"ZZs3yFk_3d-Q","colab_type":"code","colab":{}},"source":["mnist_model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x9Ip68H5TG-Z","colab_type":"text"},"source":["## Bonus: Visualize the model's layers !"]},{"cell_type":"code","metadata":{"id":"COVdYrNe68zw","colab_type":"code","colab":{}},"source":["plot_model(mnist_model, \"mnist_model.png\", show_shapes=True)\n","IPython.display.Image(  \"mnist_model.png\")"],"execution_count":0,"outputs":[]}]}
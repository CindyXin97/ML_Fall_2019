{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Putting it all together in TensorFlow/Keras\n",
    "\n",
    "# Computation graphs\n",
    "\n",
    "## Intuitive explanation\n",
    "\n",
    "When you first look at TensorFlow code, it looks like your familiar imperative program:\n",
    "- familiar operators\n",
    "    - assignment, addition, multiplication\n",
    "    - may overload operators `=`, `+`, `*`\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is **not** the same.\n",
    "\n",
    "- The statements in TensorFlow are not executed immediately (as in an imperative program)\n",
    "    - they are defining a future computation (the \"computation graph\")\n",
    "    - think of it a defining the *body* of a function\n",
    "        \n",
    "- In order to evaluate (i.e., \"call\") the function ('computation graphs\")\n",
    "    - You must create a \"session\" in TensorFlow\n",
    "    - All code must be run within a session\n",
    "    - The code is evaluated by explicitly asking for something to be \"evaluated\" or \"run\"\n",
    "        - When evaluating/running: you must pass in actual values for the formal parameters (function arguments/place holders)\n",
    "We've swept some subtle but important details under the rug.\n",
    "\n",
    "\n",
    "Consider the imperative Python program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Raw Tensorflow Notebook on Drive](https://urldefense.proofpoint.com/v2/url?u=https-3A__colab.research.google.com_drive_1vwX0IbztsybVlh9oYtfpBXFLWcI-2DAatr-3Fts-3D5da7c011&d=DwMFaQ&c=slrrB7dE8n7gBJbeO0g-IQ&r=rGnWpQIpf3UyN5UxbAs9oPgrYvuF0fDXAV2g1_EuNCg&m=L4mP2aZE1cYG9qRa6ZoTwAeunh66tXR2gWEp26TZpPQ&s=502bRW0foGgB8rCia3EU58_JTAt86qhSJLP3SV0qOaU&e=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "b = 1\n",
    "c = a + b\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "and the very similar looking TensorFlow"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "a = tf.Variable(2)\n",
    "b = tf.Variable(1)\n",
    "c= tf.add(a,b)\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You are literally building a graph of information flow\n",
    "\n",
    "To actually do something, you have to \"evaluate\" part of the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  init.run()\n",
    " \n",
    "  c_value = sess.run(c)\n",
    "  print(\"c value:\", c_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the imperative program, each line is evaluated immediately after it is executed.\n",
    "\n",
    "In the declarative program, it is not evaluated: it just creates a dependence between outputs (c) and inputs (a and b). When you evaluate c, it recursively evaluates all the things that c depends on.\n",
    "\n",
    "Hence, you are declaring a graph that is evaluated later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computation graph: a node is an expression, not a value\n",
    "\n",
    "Imagine that a variable has two attributes\n",
    "- `c.value`: the current \"value\" of the variable\n",
    "- `c.expr` - the expression that computes `c`\n",
    "\n",
    "When we write\n",
    ">`c = a + b`\n",
    "\n",
    "in our familiar imperative programming languages, this really denotes the imperative\n",
    ">`c.value = a.value + b.value`\n",
    "\n",
    "That is, the string `c = a + b` is a *command* to modify the value of `c`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In a declarative program, the string `c = a + b` defines a *function* that computes `c` from two\n",
    "inputs `a, b`\n",
    "\n",
    ">`c.expr = lambda a,b: plus(a,b)`\n",
    "\n",
    "Thus, it's possible to write the string `c = a + b` even before `a, b` have been initialized\n",
    "because `a, b` are just formal parameters to the function `c.expr`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In order to *evaluate* `c.expr` (i.e., compute the concrete value `c.value`) we must first evaluate\n",
    "\n",
    ">`a.expr, b.expr`\n",
    "\n",
    "Note that the declarative program distinguishes between *declaring/defining* an expression\n",
    "and *evaluating* it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "More formally, the `eval` operator (which derives a value from a function) applied to `c` results in\n",
    "\n",
    ">`eval(c.expr) = plus( eval(a.expr), eval(b.expr) )`\n",
    "\n",
    "These in turn might be expressions that depend on other expressions, e.g., \n",
    ">`a.expr = lambda d, e: mult(d,e)`\n",
    "\n",
    "So the evaluation of the top-level expression `c.expr` involves recursively evaluating all\n",
    "expressions on which `c.expr` depends.\n",
    "Eventually the recursion unwinds to a base case in which the expression involves no further computation\n",
    "\n",
    ">`d = lambda: d.value`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As we traverse the code of the declarative program, we are defining more and more functions,\n",
    "and dependencies between functions (i.e., some functions consume the results of other functions as arguments).\n",
    "\n",
    "This collection of functions is called a *computation tree*.\n",
    "A computation tree is just a collection of functions and dependencies.\n",
    "A node `c` in the tree has *no concrete* value until we request it to be *evaluated*, which\n",
    "involves \n",
    "- binding concrete values to all leaf nodes of the sub-tree defining `c.expr`\n",
    "- recursively evaluating the nodes on which `c` depends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Eager execution\n",
    "\n",
    "Many people find declarative programming confusing (and perhaps pointless).\n",
    "\n",
    "As you will see, there is a point (and a very big one. Hint: do you like to write derivatives ?)\n",
    "\n",
    "TF supports \"eager execution\" which makes TF look like an imperative language. This is optional in TF v1, and standard in TF v2.\n",
    "\n",
    "So, when reading other people's code, it's important to observe whether eager execution has been enabled.\n",
    "\n",
    "TF v2 is not yet standard so most code you will currently see is declarative.\n",
    "\n",
    "You may stumble at first, but it is very powerful.\n",
    "\n",
    "[Introducing Eager execution](https://developers.googleblog.com/2017/10/eager-execution-imperative-define-by.html?source=post_page---------------------------)\n",
    "- Because you are not building a graph, the training loop is different\n",
    "    - more Pythonic\n",
    "    - no need to\n",
    "        - instantiate session\n",
    "        - `eval` or `run` the training step\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "for (x, y) in tfe.Iterator(dataset):\n",
    "  grads = tfe.implicit_gradients(loss_function)(model, x, y)\n",
    "  optimizer.apply_gradients(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deeper dive: inside the fully connected layer\n",
    "\n",
    "Before Keras there was a layer API.\n",
    "\n",
    "Before the layer API, there was raw TF.\n",
    "\n",
    "Because you can find multiple generations of code on the web, it can be confusing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A lot of code that you will find still uses raw TF.\n",
    "\n",
    "And, on occassion, you have to write some raw TF: either in a Lambda function (like your own layer) or\n",
    "    when the connections between layers are more complex than the Sequential Keras model\n",
    "- e.g., a model that takes inputs from the outputs of two other models\n",
    "    \n",
    "So let's implement a FC layer in raw TF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Keras\n",
    "\n",
    "[Keras](https://keras.io/) is a high level API for Neural Networks.\n",
    "It supports multiple NN engines (\"back ends\") including TensorFlow, Theano, and CNTK.\n",
    "So you can write a single program in Keras and run it on different underlying engines.\n",
    "\n",
    "We will be using TensorFlow as our engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Technically speaking: Keras is an API -- a specification -- not a library.\n",
    "TensorFlow has implemented this specification within the TensorFlow language\n",
    "\n",
    "**This is not just a legal difference**\n",
    "- Keras is available as a separate package, independent of TensorFlow\n",
    "-` tensorflow.keras` is the Keras API implemented (and well-integrated) into TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This may get confusing\n",
    "- when you read the [Keras docs](https://keras.io/) they are referring to the abstract Keras API\n",
    "    - used as `import keras as keras`\n",
    "\n",
    "- when you read the [TensorFlow docs for Keras](https://www.tensorflow.org/guide/keras) it is refering to TensorFlows implementation of the API\n",
    "    - used as `from tensorflow import keras`\n",
    "    - this is what we will use !\n",
    "\n",
    "[TensorFlow 2.0](https://medium.com/tensorflow/standardizing-on-keras-guidance-on-high-level-apis-in-tensorflow-2-0-bad2b04c819a)  (currently in beta release) has chosen to implement Keras as its (exclusive ?) high level API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Boiler plate: Guidance from TensorFlow team\n",
    "[Guidance from TensorFlow team](https://medium.com/tensorflow/standardizing-on-keras-guidance-on-high-level-apis-in-tensorflow-2-0-bad2b04c819a)\n",
    "- `tf.keras` is an implementation of the Keras API\n",
    "    - *with enhancements*\n",
    "        - eager execution\n",
    "    - integrated into TensorFlow ecosystem\n",
    "        - `tf.data`\n",
    "    - use as\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "Dense = tf.keras.layers.Dense\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(64, activation=’relu’))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "although on the same page they *also* use\n",
    "- `tf.keras.models.Sequential` rather than `tf.keras.Sequential`\n",
    "- `tf.keras.layers.Dense`\n",
    "    - presumably depending on `from tensorflow.keras import layers`\n",
    "    \n",
    "** SO GUESS ** that file path `tensorflow.keras` is same module as `tf.keras`\n",
    "\n",
    "Can actually show this in a notebook:\n",
    "- `import tensorflow.keras.layers as tkl`\n",
    "    - `tkl.Dense??` and `tf.keras.layers.Dense??` wind up in same file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Eliminate the doubt: which functions are the same \n",
    "\n",
    "When you see sample code for different places, the same function may be referred to with slightly different\n",
    "names; just the way Python imports work.\n",
    "\n",
    "What **is** crucial to distinguish is TensorFlow's implementation of Keras and the Keras API\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# \"keras\" prefix will refer to tensorflow.keras\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Also import the Keras API, for comparison\n",
    "import keras as k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "All the following comparisons evaluate to True, as you can test for yourself"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "tf.keras.layers.Dense == keras.layers.Dense\n",
    "tf.keras.layers.Dense == layers.Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But the following is **not** True because they come from different packages:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "tf.keras.layers.Dense == k.layers.Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Getting Started with TensorFlow](https://www.tensorflow.org/tutorials?source=post_page---------------------------)\n",
    "\n",
    "[Notebook in Colab](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/_index.ipynb?source=post_page---------------------------#scrollTo=hiH7AC-NTniF)\n",
    "\n",
    "Sample good (can see modules used)\n",
    "- `import tensorflow as tf`\n",
    "- everything else is from `tf`\n",
    "    - `tf.keras.models.Sequential`\n",
    "    - `tf.keras.layers.Dense`\n",
    "    - `tf.keras.datasets.mnist`"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Restoring order to the world\n",
    "\n",
    "\n",
    "\n",
    "Having Keras tightly integrated into TensorFlow 2.0 cleans up a rather unruly TensorFlow eco-system that resulted in  similar functionality in multiple places.\n",
    "\n",
    "This can make it very confusing for someone new to TensorFlow.  There are lots of examples on the web written\n",
    "using various similar-looking packages. \n",
    "\n",
    "I'll try to point out potential sources of confusion. Beware !\n",
    "\n",
    "[Demystify the TensorFlow APIs](https://medium.com/google-developer-experts/demystify-the-tensorflow-apis-57d2b0b8b6c0) summarizes it well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note that `keras` and `tf.keras` are *two different name spaces!*\n",
    "\n",
    "- `tf.keras` vs`keras`\n",
    "    - `'keras` is a separately installed package (`pip install keras`)\n",
    "        - used as `import keras as keras`\n",
    "        - you can choose any engine\n",
    "        - this is the \"use Keras with TensorFlow as a back end\" option\n",
    " \n",
    "    - `tf.keras` is the TensorFlow implementation of the Keras API\n",
    "        - **recommended** vs the Keras package\n",
    "            - better integrated into TensorFlow\n",
    "        \n",
    "        - used as `from tensorflow import keras`\n",
    "            - subsequently, call `keras.layers.Dense` etc\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- `tf.layers` is going away in TensorFlow 2.0\n",
    "    - `tf.keras` is recommended going forward\n",
    "    - **Do not use**\n",
    "    \n",
    "- [Estimators](https://www.tensorflow.org/guide/estimators) (`tf.Estimator`)\n",
    "    - Estimators are sometimes called \"models in a box\"; somewhat similar to `sklearn`\n",
    "        - pre-canned high-level models (like Classifiers) rather than low-level `tf.keras.layers` (like Dense) from which it is built\n",
    "        - convenient interface to [Datasets for Estimators](https://www.tensorflow.org/guide/datasets_for_estimators)\n",
    "            - no need to create own mini-batches, etc.\n",
    "    - You can achieve quite a bit of the convenience using Keras, so we will skip Estimators.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Low-level TensorFlow \n",
    "    - great for learning\n",
    "    - better to rely on pre-defined layers when possible\n",
    "    \n",
    "And our own observations\n",
    "- `tf.contrib`\n",
    "    - this was a name-space created to enable users to contribute useful packages.\n",
    "    - some of these packages may have made their way into the core, or been integrated elsewhere\n",
    "        - `tf.contrib.learn.Estimator` is the obsolete version of `tf.Estimator`\n",
    "    - eliminated from TensorFlow 2.0\n",
    "        - **avoid**\n",
    "- [Datasets API](https://www.tensorflow.org/guide/datasets)\n",
    "    - an API to handle large datasets, in memory- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will focus on two styles or packages\n",
    "- `tf.keras`\n",
    "    - this is the future, as it will be tightly integrated into TensorFlow 2.0\n",
    "- `tf.layers` modules (e.g., `tf.layers.dense`)\n",
    "    - used only to be compatible with the Geron book.\n",
    "    - it is slightly lower level than Keras\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are two \"API\"'s\" `Sequential` and `Functional`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sequential\n",
    "[Getting started with the Keras Sequential Model](https://keras.io/getting-started/sequential-model-guide/)\n",
    "\n",
    "Specify a model as a sequence of layers.  Very natural\n",
    "\n",
    "**NOTE** copying from Keras (rather than tf.keras) so really should prefix with\n",
    "`import tf.keras as keras`\n",
    "or\n",
    "`from tensorflow import keras``"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(32, input_shape=(784,)),\n",
    "    Activation('relu'),\n",
    "    Dense(10),\n",
    "    Activation('softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The only limitation is that your computation graphs is a non-branching line (functions called in sequence)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Functional\n",
    "\n",
    "It is a little more verbose than `Sequential` but also more flexible in that you can define more complex computation graphs (multiple inputs/outputs, shared layers)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# This returns a tensor\n",
    "inputs = Input(shape=(784,))\n",
    "\n",
    "# a layer instance is callable on a tensor, and returns a tensor\n",
    "x = Dense(32, activation='relu')(inputs)\n",
    "predictions = Dense(10, activation='softmax')(x)\n",
    "\n",
    "# This creates a model that includes\n",
    "# the Input layer and  Dense layers\n",
    "model = Model(inputs=inputs, outputs=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Notice that you are (manually) invoking a single layer at a time, passing as input the output of the prior layer.\n",
    "\n",
    "- You must define an `Input` layer (placeholder for the input)\n",
    "    - in `Sequential` you instead give an `input_shape=` parameter to the first layer to specify input shape)\n",
    "- You \"wrap\" the graph into a \"model\" by a `Model` statement\n",
    "    - looks like a function definition\n",
    "        - names the input and output formal parameters\n",
    "    - a `Model` acts just like a layer (but with internals that you create)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(data, labels)  # starts training"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

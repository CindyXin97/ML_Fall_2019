{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro `_latex_std_` created. To execute, type its name (without quotes).\n",
      "=== Macro contents: ===\n",
      "get_ipython().run_line_magic('run', 'Latex_macros.ipynb')\n",
      " "
     ]
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "\n",
    "Up until now, the layers we have studed (Dense, Convolution) are primarily used to implement functions\n",
    "that are one to one: \n",
    "- a single input (of fixed length) yields a single output.\n",
    "\n",
    "Recurrent Neural Networks (RNN) deal with sequences: \n",
    "- sequences of inputs and sequence of outputs.\n",
    "\n",
    "Sequences have order: a permutation of the elements of a sequence is a completely distinct sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Order matters !\n",
    "\n",
    "Order matters\n",
    "- set $\\{ \\x_{(1)} \\dots \\x_{(\\tt-1)}\\}$ versus sequence $\\x_{(1)} \\dots \\x_{(\\tt-1)}$\n",
    "    - order doesn't matter for a set\n",
    "    - $\\{ \\x_{(1)} \\dots \\x_{(\\tt-1)}\\} = \\{  \\x_{\\tt-1)} \\ldots \\x_{(1)}\\}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Same prices</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_sequence_1.jpg\" width=800></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>Same words</center>\n",
    "$$\n",
    "\\begin{matrix}\n",
    "\\text{Machine} & \\text{Learning} & \\text{is} & \\text{easy} & \\text{not} & \\text{difficult} \\\\\n",
    "\\text{Machine} & \\text{Learning} & \\text{is} & \\text{difficult} & \\text{not} & \\text{easy} \\\\\n",
    "\\end{matrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Same pixels</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_sequence_2.jpg\" width=800></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "All pairs of inputs in above examples are\n",
    "- identical as sets\n",
    "- different as sequences\n",
    "\n",
    "**Note**\n",
    "\n",
    "If the paired examples have different labels:\n",
    "- NN will find as subset of features that separate them\n",
    "- the separating feature then becomes an anchor, restricting reordering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Functions on sequence\n",
    "\n",
    "Examples of functions with sequence as inputs but single output (many to one mapping)\n",
    "- predict next value in time series\n",
    "- predict sentiment of text\n",
    "\n",
    "Examples of functions with single input but sequence as output (one to many mapping)\n",
    "- text generation (char-rnn) ?\n",
    "\n",
    "Examples of functions with sequence as inputs and outputs\n",
    "- translating from one language to another\n",
    "- captioning a movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Notation alert**\n",
    "\n",
    "Lot's of dimenions !\n",
    "- $\\x^\\ip$ is a sequence with elements $\\x^\\ip_{(1)} \\dots \\x^\\ip_{(\\tt-1)}$\n",
    "- each element $\\x^\\ip_\\tp$ is a vector\n",
    "    - $x^\\ip_{(\\tt),j}$ is a feature\n",
    "        - examples\n",
    "            - $\\x^\\ip_\\tp$ is a frame $\\tt$ in a movie; $x^\\ip_{(\\tt),j}$ is a pixel in frame $\\tt$\n",
    "            - $\\x^\\ip_\\tp$ are the characteristics of a stock on day $\\tt$, $x^\\ip_{(\\tt),j}$ is price or volume\n",
    "            - $\\x^\\ip_\\tp$ is a word $\\tt$ in a sentence; $x^\\ip_{(\\tt),j}$ is element $j$ of the OHE of the word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Choices for how to predict $\\y_\\tp$ that is dependent on $\\x_{(1)} \\dots \\x_\\tp$\n",
    "\n",
    "$$\n",
    "\\begin{array}[lll] \\\\\n",
    "\\pr{\\y_\\tp | \\x_{(1)} \\dots \\x_\\tp}  & \\text{direct dependence on entire prefix } \\x_{(1)} \\dots \\x_\\tp \\\\\n",
    "\\\\\n",
    "\\pr{\\h_\\tp | x_\\tp, \\h_{(\\tt-1)} } & \\text{latent variable } \\h_\\tp \\text{encodes } \\x_{(1)} \\dots \\x_\\tp \\\\\n",
    "\\pr{\\y_\\tp | \\h_\\tp }              & \\text{prediction contingent on latent variable} \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The Recurrent Neural Network (RNN) adopts the latter approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The single layer may also emit an output at step $i$ (for outputs that are sequences).\n",
    "\n",
    "Here is some pseudo-code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def RNN( input_sequence, state_size ):\n",
    "    state = np.random.uniform(size=state_size)\n",
    "    \n",
    "    for input in input_sequence:\n",
    "        # Consume one input, update the state\n",
    "        out, state = f(input, state)\n",
    "        \n",
    "    return out\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_loop.jpg\" width=1000></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that RNN's are sometimes drawn without separate outputs $\\y_t$\n",
    "- in that case, $\\h_t$ may be considered the output. \n",
    "\n",
    "The computation of $\\y_\\tp$ will be just a linear transformation of $\\h_t$ so there is no loss in omitting\n",
    "it from the RNN and creating a separate node in the computation graph.\n",
    "\n",
    "Geron does not distinguish betwee $\\y_t$ and $\\h_t$ and he uses the single $\\y_\\tp$ to denote the state.\n",
    "\n",
    "I will use $\\h$ rather than $\\y$ to denote the \"hidden state\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## $\\h_\\tp$ latent state\n",
    "\n",
    "- $\\h$ is a *fixed length* encoding of variable length sequence $\\x_{(1)} \\dots $\n",
    "    - $\\h_\\tp$ encodes $\\x_{(1)} \\dots \\x_\\tp$\n",
    "    - gives a way to have variable length input to, e.g., classifiers\n",
    "- $\\h_\\tp$ is a vector of features\n",
    "    - captures multiple \"dimensions\"/concepts of the input sequence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN Many to one; followed by classifier</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_many_to_one_to_classifier.jpg\" width=800></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unrolling a loop\n",
    "We can \"unroll\" the loop into the sequence of steps, with time as the horizontal axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN unrolled</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Unrolled_RNN.jpg\" width=800></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that $\\x, \\y, \\h$ are all vectors. \n",
    "\n",
    "In particular, the state $\\h$ may have many elements\n",
    "-  to record information about the entire prefix of the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The key connection is that the state at time $t-1$ is passed as input to time $t$.\n",
    "\n",
    "So when processing $\\x_\\tp$\n",
    "- the layer can take advantage of a summary ($\\h_{(t-1)}$) of every input that preceded it.\n",
    "\n",
    "One can look at this unrolled graph as being a dynamically-created computation graph.\n",
    "\n",
    "Essentially, each state $\\h$ is replicated per time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This view of the computation graph is important\n",
    "- it shows you the exact computation\n",
    "- it should tell you how gradients are computed\n",
    "    - in particular, the loss and gradients flow backwards\n",
    "        - so the gradients involving $\\h$ are updated at *each* time step.\n",
    "        - this will be important when we discuss\n",
    "            - vanishing/exploding gradients\n",
    "            - skip connections\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The RNN API\n",
    "\n",
    "During one time step of computation, the RNN computes 2 values\n",
    "- new  state $\\h_\\tp$\n",
    "- output $\\y_\\tp$ (sometimes simply taken to be same as shor term state\n",
    "\n",
    "The state computation is a function of the previous state $\\h_{(t-1)}$,\n",
    "and the current input $x_\\tp$.\n",
    "\n",
    "$$\n",
    "\\h_\\tp = f(\\x_\\tp;  \\h_{(t-1)})\n",
    "$$\n",
    "\n",
    "Note the recursive aspect of the computation of  $\\h_\\tp$: \n",
    "- it implicitly depends\n",
    "on the values of the states at all previous time steps $t' < t$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RNN as a layer\n",
    "\n",
    "## Many to one\n",
    "\n",
    "Although the unrolled RNN looks confusing, as an \"API\" the RNN just acts as any other layer\n",
    "- takes some input $\\x$ (which happends to be a sequence)\n",
    "- produces a single output\n",
    "\n",
    "If we draw a box around the unrolled RNN, we can see the \"API\":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN many to one</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_layer_many_one.jpg\" width=800></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RNN layer as an encoder\n",
    "The many to one RNN essentially creates a compact encoding of an arbitrarily long sequence.\n",
    "\n",
    "This can be very useful as we can feed this \"summary\" (representation) of the entire sequence\n",
    "into layers that can't handle sequence inputs.\n",
    "\n",
    "Note that there is nothing special about a layer creating a compact encoding (representation) of it's input.\n",
    "\n",
    "A CNN layer, with outputs flattened to one dimension, creates a compact encoding of an image.\n",
    "\n",
    "The real power of the RNN is the ability to encode all sequences, regardless of length, into a fixed size\n",
    "representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Sequences: variable length input summarized\n",
    "\n",
    "$\\h_\\tp$ summarizes the length $\\tt$ sequence $\\x_{1,\\ldots, \\tt}$ in a *fixed size* vector $\\h_\\tp$.\n",
    "- makes sequences amenable to models that can only deal with fixed size input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To be clear\n",
    "- the RNN is a layer, just like any other\n",
    "    - Internally it implements a loop but that is ordinarily hidden\n",
    "    - The intuition about the \"unrolled loop\" is to help us to better understand the inner workings, not as a coding matter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Like any other layer, it produces an output (although after multiple time steps for an RNN versus\n",
    "a single time step for a Dense layer).\n",
    "                                          \n",
    "- If the length of sequence $\\x$ is $T$, there is ordinarily a **single** output $\\y_{(T)}$\n",
    "    - $\\y_{(T)}$ is only available after the entire input sequence has been consumed\n",
    "    - the intermediate results \n",
    "    $$\\h_\\tp, \\y_\\tp, \\; t = 1, \\ldots, (T-1)$$ \n",
    "    are not visible through the API\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Many to many\n",
    "\n",
    "The above behavior defines a many to one mapping from input sequence (many) to single output (one).\n",
    "\n",
    "With a minor change, we can define a many to many mapping:\n",
    "- each element of the input sequence\n",
    "results in one element of an output sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Many Deep Learning software API's will see recurrent layers with an optional\n",
    "argument \n",
    "- `return_sequences`\n",
    "- `return_states` \n",
    "- both default to `False` in Keras.\n",
    "\n",
    "This controls the output behavior of the RNN layer, whether it returns one output per time step\n",
    "$$\n",
    "       \\h_{(1)}, \\ldots, \\h_{(T)} \\\\\n",
    "       \\y_{(1)}, \\ldots, \\y_{(T)}\n",
    "$$\n",
    "or just\n",
    "$$\n",
    "\\h_{(T)} \\\\\n",
    "\\y_{(T)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is how any RNN behaves when the function it's implementing is many to many:\n",
    "- one output per time step.\n",
    "\n",
    "When the RNN needs to implement a many to one function, the layer looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN many to many</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_layer_many_many.jpg\" width=800></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The art-work needs to be clarified\n",
    "- the RNN layer produces sequences\n",
    "    - as outputs $\\y$\n",
    "    - as states $\\h$\n",
    "\n",
    "These sequences are available when the RNN layer *completes* its consumption of input $\\x$.\n",
    "\n",
    "The following diagram may clarify\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN many to many, clarified</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_loop_many_many.jpg\" width=1000></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- the `return_sequences` argument instructs the layer to produce a sequence $\\y$\n",
    "    - rather than a scalar, as in the many to one case\n",
    "- the `return_states` argument instructs the layer to return the state $\\h$ as well\n",
    "    - useful if we stack RNN layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stacked RNN layers\n",
    "\n",
    "One can connect RNN layers into \"stacks\" \n",
    "- by feeding\n",
    "the output state of one RNN layer as the input to the successor layer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN Stacked layers</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_layers_stacked.jpg\" width=800></td>\n",
    "    </tr>\n",
    "</table>\n",
    "​"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Encoder/Decoder architecture\n",
    "\n",
    "An *Encoder/Decoder* is a two part Neural Network that is applied to many NLP tasks\n",
    "- *Encoder* converts sequence (sentence) into intermediate representation (sequence)\n",
    "- *Decoder* converts intermediate sequence to final sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN Encoder/Decoder</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_Encoder_Decoder.jpg\" width=800></td>\n",
    "    </tr>\n",
    "</table>\n",
    "​\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sequence to Sequence\n",
    "\n",
    "- Many to one encoder\n",
    "    - variable length input to fixed length final state $\\h$\n",
    "- One to one decoder\n",
    "    - *initial* state of decoder set to *final* state of encoder\n",
    "    - teacher forcing\n",
    "        - input $(\\tt +1)$ of decoder is out $\\tt$ of decoder\n",
    "\n",
    "Example: language translation\n",
    "\n",
    "This is useful when there is not an exact one to one correspondence between tokens in the source and target languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training: forward/backward pass, cost/loss\n",
    "\n",
    "Examples\n",
    "- an example $\\x^\\ip$ is now a *sequence* $\\x^\\ip_{(1)}, \\x^\\ip_{(2)}, \\ldots, \\x^\\ip_{(T)} $\n",
    "    - variable length\n",
    "    - $\\x^\\ip_\\tp$ *may* be a vector (doesn't have to be scalar), \n",
    "        - e.g., word embedding\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Per example loss $\\loss^\\ip$ *per time step*\n",
    "- In many to many: there is a loss per time-step.  \n",
    "- Total loss (over which we optimize) is sum, orver time , of the loss per time step\n",
    "    - $\\loss^\\ip = \\sum_{\\tt=1}^n \\loss^\\ip_\\tp$\n",
    "- In many to one: loss is single value (per example): depends on final state \n",
    "    - $\\loss^\\ip = \\loss_{(T)}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RNN details: update equations\n",
    "\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\h_\\tp & = & \\phi(\\W_{xh}\\x_\\tp  + \\W_{hh}\\h_{(t-1)}  + \\b_h) \\\\\n",
    "\\y_\\tp & = &  \\W_{hy} \\h_\\tp  + \\b_y \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "where $\\phi$ is an activation function (usually $\\tanh$)\n",
    "\n",
    "**Note** Geron prefers right multiplying weights $\\x_\\tp \\W_{xh}$ versus $\\W_{xh}\\x_\\tp$\n",
    "- left multiplying seems more common in literature\n",
    "\n",
    "**Note**\n",
    "The equation is for a single example.  \n",
    "\n",
    "In practice, we do an entire minibatch so have $m$ $\\x's$\n",
    "given as a $(m \\times n)$ matrix $\\mathbf{X}$.\n",
    "\n",
    "**page 471: mention dimensions of each**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Equation in pseudo-matrix form\n",
    "\n",
    "You will often see a short-hand form of the equation.\n",
    "\n",
    "Look at $\\h_\\tp$ as a function of two inputs $\\x_, \\h_{(t-1)}$.\n",
    "\n",
    "We can stack the two inputs into a single matrix.\n",
    "\n",
    "Stack the two matrices $\\W_{xh}, \\W_{hh}$ into a single weight matrix\n",
    "\n",
    "$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\h_\\tp  = \\W \\mathbf{I} + \\b \\\\\n",
    "\\text{ with } \\\\\n",
    "\\W = \\left[\n",
    " \\begin{matrix}\n",
    "    \\W_{xh} & \\W_{hh}\n",
    " \\end{matrix} \n",
    " \\right] \\\\\n",
    "\\mathbf{I} = \\left[\n",
    " \\begin{matrix}\n",
    "    \\x_\\tp  \\\\\n",
    "    \\h_{(t-1)}\n",
    " \\end{matrix} \n",
    " \\right] \\\\\n",
    "\\end{array}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Stacked RNN layers revisited\n",
    "\n",
    "With the benefit of the RNN update equations, we can clarify how stack RNN layers works>\n",
    "\n",
    "Let superscript $[\\ll]$ denote a stacked layer of RNN.\n",
    "\n",
    "So the RNN update equation for the bottom layer $1$ becomes\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\h^{[1]}_\\tp & = & \\phi(\\W_{xh}\\x_\\tp  + \\W_{hh}\\h^{[1]}_{(t-1)}  + \\b_h) \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The RNN update equation for leyer $[\\ll]$ becomes\n",
    "\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\h^{[\\ll]}_\\tp & = & \\phi(\\W_{xh}\\h^{[\\ll-1]}_\\tp  + \\W_{hh}\\h^{[\\ll]}_{(t-1)}  + \\b_h) \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "That is: the input to layer $[\\ll]$ is $\\h^{[\\ll-1]}_\\tp$ rather than $\\x_\\tp$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Back propagation through time (BPTT)\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>TL;DR</b> \n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>We can \"unroll\" the RNN into a sequence of layers, one per time step</li>\n",
    "        <li>In theory: Back Propagation on the unrolled RNN is the same as for a non-Recurrent Network</li>\n",
    "        <li>In practice: the unrolled RNN is very deep, which causes issues in Back Propagation.\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*Back Propagation Through Time (BPTT)* refers to\n",
    "- unrolling the RNN computation into a sequence of layers\n",
    "- performing ordinary Back Propagation in order to update weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- In a non-Recurrent network:\n",
    "    - $\\W_\\llp$, the weights of layer $\\ll$, affect only layers $\\ll$ and greater.\n",
    "    - This means the backward flow of the gradient with respect to $\\W_\\llp$ stops at layer $\\ll$.\n",
    "- In  Recurrent Network:\n",
    "    - All unrolled \"layers\" share the *same* weights\n",
    "    - This means the gradients with respect to shared weight $\\W$ must flow backward all the way to the input layer at time $0$.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN Loss Gradient Flow</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_layer_loss_gradient.jpg\" width=800></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The unrolled graph is as deep as the length of $\\x^\\ip$ ($T^\\ip = |\\x^\\ip|)$ \n",
    "- weights can update only after $T^\\ip$ input values have been processed,\n",
    "so training can be slow.\n",
    "- Vanishing Gradients become a concern for large $T^\\ip$\n",
    "    - Recall from the Vanishing Gradient lecture: magnitude of gradients diminishes from layer $\\ll$ to layer $(\\ll-1)$ during back propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Calculating gradients with BPTT\n",
    "\n",
    "### Back propagation: Refresher\n",
    "\n",
    "The same math that we used to show how to obtain derivatives (for weight updates in Gradient Descent)\n",
    "will apply to RNN's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To refresh our memory on notation and results, recall our derivation of back propagation:\n",
    "\n",
    "Layer $\\ll$:\n",
    "- input/output relation of layer $\\ll$ as\n",
    "$$\n",
    "\\y_{(\\ll)} = a_{(\\ll)}( f_{(\\ll)}( \\y_{(\\ll-1)}, \\W_{(\\ll)}) )\n",
    "$$\n",
    "\n",
    "for\n",
    "- activation function $a_\\llp$\n",
    "- weights $\\W_\\llp$\n",
    "- $\\y_{(\\ll-1)}$ are the outputs of the previous layer\n",
    "\n",
    "- $f_{(\\ll)}$ is the function computed by layer $\\ll$\n",
    "    - function of input $\\y_{(\\ll-1)}$ and weights $\\W_\\llp$\n",
    "    - e.g., `Dense`: $f_{(\\ll)}( \\y_{(\\ll-1)}, \\W_{(\\ll)}) = \\y_\\llp = \\W_\\llp \\y_{(\\ll-1)} + \\b_\\llp$\n",
    "\n",
    "**Note** We neglect to add $\\b_\\llp$ as an argument to $f_\\llp$ to simplify notation\n",
    "- as a convenience we sometimes view $\\b_\\llp$ as being part of $\\W_\\llp$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let \n",
    "- $\\loss$ denote loss (computed after final layer $L$)\n",
    "- $\\loss'_\\llp = \\frac{\\partial \\loss}{\\partial y_\\llp}$ denote the derivative of $\\loss$ with respect to the output of layer $\\ll$, i.e., $y_\\llp$,\n",
    "    - refer to as **loss gradient** (at output of layer $\\ll$)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We showed how to compute\n",
    "- $\\loss'_{(\\ll-1)}$ from $\\loss'_\\ll$ \n",
    "    - so that we can continue this process as the previous layer (i.e, *propogate loss gradient backwards*)\n",
    "\n",
    "and we showed how to compute the weight update\n",
    "- $\\frac{\\partial \\loss}{\\partial W_\\llp}$, from $\\loss'_\\llp$  for $\\ll \\in [1,L]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that $\\y_\\llp$ is a function of \n",
    "- $\\y_{(\\ll-1)}$ (the output of the previous layer) \n",
    "- and $\\W_\\llp$, the parameters of layer $\\ll$.\n",
    "\n",
    "We can compute derivatives of $\\y_\\llp$ with respect to each of its inputs\n",
    "- $\\frac{\\partial \\y_\\llp}{\\partial \\y_{(\\ll-1)}}$\n",
    "- $\\frac{\\partial \\y_\\llp}{\\partial \\W_\\llp}$\n",
    "\n",
    "Refer to these as **local gradients**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We used the chain rule to obtain the \n",
    "- gradient with respect to weights $\\W_\\llp$, given the loss gradient $\\loss'_\\llp$ \n",
    "\n",
    "$$\n",
    "\\begin{array}[lll] \\\\\n",
    "\\frac{\\partial \\loss}{\\partial \\W_\\llp} & = & \\frac{\\partial \\loss}{\\partial \\y_\\llp} \\frac{\\partial \\y_\\llp}{\\partial \\W_\\llp} & = & \\loss'_\\llp \\frac{\\partial \\y_\\llp}{\\partial \\W_\\llp}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "That is: \n",
    "- gradient of $\\loss$ with respect to weight $\\W_\\llp$ \n",
    "- is the loss gradient (at current step), multiplied by\n",
    "- a local gradient (with respect to input  $W_\\llp$ )\n",
    "\n",
    "So we have the information required to update $\\W_\\llp$ by Gradient Descent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### BPTT: gradient calculation\n",
    "\n",
    "Let us adapt these results for the case of a *single layer* RNN\n",
    "- by \"unrolling\" this RNN, layer $\\ll$ is equated with \"time\" (of index into input sequence ) $\\tt$\n",
    "\n",
    "Per example loss $\\loss^\\ip$ is now a per example loss *per time step*\n",
    "$$\n",
    "\\loss^\\ip_\\tp\n",
    "$$\n",
    "so\n",
    "$$\n",
    "\\loss^\\ip = \\sum_{t=1}^T \\loss^\\ip_\\tp\n",
    "$$\n",
    "\n",
    "We will focus on the per example loss for a single time $\\loss^\\ip_\\tp$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "​\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN Loss</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_layer_loss.jpg\" width=800></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compute \n",
    "$$\n",
    "\\frac{\\partial \\loss^\\ip}{\\partial \\W}\n",
    "$$\n",
    "we must compute\n",
    "$$\n",
    "\\frac{\\partial \\loss^\\ip_\\tp}{\\partial \\W}\n",
    "$$\n",
    "for each time $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As per regular backprop, we can obtain the loss update by\n",
    "multiplying the loss gradient by a local gradient\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\loss^\\ip_\\tp}{\\partial \\W}\n",
    "$$\n",
    "\n",
    "but note that we use unsubscripted $\\W$ (rather than $\\W_\\tp$ because the *same* $\\W$ is used at all timesteps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\n",
    "\\frac{\\partial \\loss^\\ip_\\tp}{\\partial \\W} = \\loss'_\\tp \\frac{\\partial \\y_\\tp}{\\partial W}\n",
    "$$\n",
    "\n",
    "but now \n",
    "$$\n",
    "\\frac{\\partial \\y_\\tp}{\\partial W}\n",
    "$$\n",
    "\n",
    "becomes more complicated, governed by the RNN Update equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\h_\\tp & = & \\phi(\\W_{xh}\\x_\\tp  + \\W_{hh}\\h_{(t-1)}  + \\b_h) \\\\\n",
    "\\y_\\tp & = &  \\W_{hy} \\h_\\tp  + \\b_y \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "**Notes**\n",
    "\n",
    "- In this section we will assume $\\phi$ is the identity function to simplify the presentation.\n",
    "\n",
    "    - There will be no loss of generality.\n",
    "- Recall that $\\W$ is the matrix with embedded sub-matrices $\\W_{xh}, \\W_{hh}, \\W_{hy}$\n",
    "    - For clarity: we will add subscripts to $\\W$ in the derivatives to show which part of $\\W$ is the cause."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The equation defining $\\y_\\tp$\n",
    "$$\n",
    "\\y_\\tp  =   \\W_{hy} \\h_\\tp  + \\b_y \n",
    "$$\n",
    "\n",
    "shows that $\\y_\\tp$ is\n",
    "- directly depends on  $\\W$ (through $\\W_{hy}$ )\n",
    "- *and* indirectly depends on $\\W$  through its dependence on $h_\\tp$ (which depends on $\\W$)\n",
    "\n",
    "So\n",
    "$$\n",
    "\\frac{\\partial \\y^\\ip_\\tp}{\\partial \\W} = \n",
    "\\frac{\\partial \\y^\\ip_\\tp}{\\partial \\W_{hy}}\n",
    "+\n",
    "\\frac{\\partial \\y^\\ip_\\tp}{\\partial \\h_{(\\tt)} } \\frac{\\partial \\h_{(\\tt)}}{\\partial \\W_{hh}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's expand the term\n",
    "$$\n",
    "\\frac{\\partial \\h_{(\\tt)}}{\\partial \\W_{hh}}\n",
    "$$\n",
    "\n",
    "Recall the recursive definition of $\\h_\\tp$ \n",
    "$$\n",
    "\\h_\\tp =  \\W_{xh}\\x_\\tp  + \\W_{hh}\\h_{(\\tt-1)}  + \\b_h\n",
    "$$\n",
    "\n",
    "$\\h_\\tp$ depends on $\\h_{(\\tt-1)}$, which by recursion depends on $\\h_{(\\tt-2)}$ which $\\ldots$ depends on $\\h_{(0)}$.\n",
    "- and all $\\h_\\tp$ share the *same* $\\W_{hh}$.\n",
    "\n",
    "This means that $\\h_\\tp$ depends on $\\W$ through *each* $\\h_{(\\tt-k)}$ for $k=1, \\ldots, \\tt$.\n",
    "$$\n",
    "\\frac{\\partial \\h_\\tp}{\\partial \\W_{hh}} = \n",
    "\\sum_{k=1}^{\\tt} { \n",
    "\\frac{\\partial \\h_{(\\tt -k)}}{\\partial \\W_{hh}} \\frac{\\partial \\h_\\tp}{\\partial \\h_{(\\tt -k)}}      \n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So \n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\loss^\\ip_\\tp}{\\partial \\W} = \\loss'_\\tp \\frac{\\partial \\y_\\tp}{\\partial W}\n",
    "$$\n",
    "\n",
    "and\n",
    "$$\\frac{\\partial \\y^\\ip_\\tp}{\\partial W}$$\n",
    "*depends* on all time steps from $1$ to $t$.\n",
    "\n",
    "Thus, the derivative update for $\\W$ cannot be computed without the gradient (for each time step $t$)\n",
    "flowing all the way back to time step $0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Note**\n",
    "\n",
    "Directly expanding the recursion would show\n",
    "$$\n",
    "\\frac{\\partial \\h_\\tp}{\\partial \\h_{(\\tt -k)}} = \n",
    "\\prod_{k'=0}^{k-1} { \n",
    "\\frac{\\partial \\h_{(\\tt-k')}}{\\partial \\h_{(\\tt -k' -1)}}\n",
    "} \n",
    "$$\n",
    "\n",
    "It is not necessary now, but will be useful in explaining vanishing/exploding gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN Loss Gradient Flow</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_layer_loss_gradient.jpg\" width=800></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Truncated back progagation through time (TBTT)\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>TL;DR</b> \n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>We \"unroll\" the RNN into a sequence of  T layers, one per time step</li>\n",
    "        <li>We compute the loss at each time step t, for t=1 to T.</li>\n",
    "        <li>The gradient of the loss of time step t flows backward for a limited number of time steps</li>\n",
    "        <ul>\n",
    "            <li>Rather than flowing backwards al the way to time step 0</li>\n",
    "        </ul>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "This is called *Truncated BPTT (TBPTT)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The advantage of TBPTT\n",
    "- more frequent gradient updates\n",
    "\n",
    "The disadvantage\n",
    "- the loss at step $t$ won't affect **all** previous time steps (because of truncation)\n",
    "- the error signal from time $\\tt$ does not affect any time steps below $\\tt - \\tau$.\n",
    "- this means the RNN has difficulty capturing dependencies longer than $\\tau$.\n",
    "\n",
    "Consider a long piece of text\n",
    "- The first few words indicate the gender/plurality/age of the subject\n",
    "- A mis-prediction of, e.g. gender, at word $\\tau' > \\tau$ causes an error at time step $\\tau'$\n",
    "    - which can't interact with the correct gender in the first few words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that there is *no truncation* of the forward pass of the RNN !\n",
    "\n",
    "Only gradient calculations are truncated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### TBTT: Variations\n",
    "\n",
    "There are several ways to truncate the Back Propagation.\n",
    "\n",
    "We will describe them via a function $f(\\tt) = \\tt'$\n",
    "- describes the earliest time\n",
    "step affecting the gradient of $\\loss_\\tp$\n",
    "- that is, it describes the window $\\tau$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Untruncated BPTT\n",
    "    - $f(\\tt) = 0$\n",
    "- k-truncated BPTT\n",
    "    - $f(\\tt) = \\max{}(0, \\tt -k)$\n",
    "- subsequence truncated BPTT\n",
    "    - $f(\\tt) = k * \\floor{\\tt/k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What we refer to as subsequence TBTT seems to be common\n",
    "- break long sequence $\\x^\\ip$ into subsequences (chunks) of size $k$\n",
    "- feed $\\x^\\ip$ forward as usual\n",
    "    - at the end of a subsequence: \n",
    "        - immediately compute the loss gradients for all time steps within the chunk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RNN vanishing/exploding gradient problem\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>TL;DR</b> \n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>A \"single-layer RNN that has been unrolled for T time steps</li>\n",
    "        <ul>\n",
    "            <li>is mathematically equivalent to a simple NN with T layers</li>\n",
    "            <li><bold>BUT</bold> all layers share the same weights</li>\n",
    "        </ul>\n",
    "        <li>This sharing of weights leads to a problem of Vanishing/Exploding gradients</li>\n",
    "        <ul>\n",
    "            <li>Similar to the vanishing gradient problem we derived for simple NN</li>\n",
    "            <li>but with a different root cause (weight sharing)</li>\n",
    "        </ul>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>TL;DR</b> \n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>Why shared weights are different</li>\n",
    "        <ul>\n",
    "            <li>Output <bold>y</bold> at time t is a function of cell state h at time t</li>\n",
    "            <li>Cell state h at time t is recursively defined</li>\n",
    "            <ul>\n",
    "                <li>So it is a function of cell states over all times t' < t as well</li>\n",
    "                <li>This means the weight update involves a repeated product: (t -t') times</li>\n",
    "                <li>This product tends to 0 (vanishing) or infinity (explode) as (t -t') increases</li>\n",
    "            </ul>\n",
    "            <li>So losses at time step t have difficulty updating gradients for the distant past<</li>\n",
    "            <li>RNN has difficulty with long-term dependencies</li>   \n",
    "        </ul>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Returning to the loss gradient\n",
    "we encountered the terms\n",
    "\n",
    "$$\\frac{\\partial \\y^\\ip_\\tp}{\\partial \\W}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will focus on the part of $\\W$ that is $\\W_{hh}$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\y_\\tp}{\\partial W_{hh}} = \\frac{\\partial \\y_\\tp}{\\partial \\h_\\tp} \\frac{{\\partial \\h_\\tp}}{\\partial \\W_{hh}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But recursively defined $\\h_\\tp$ is a function of $\\h_{(\\tt-1)}, \\h_{(\\tt-1)}, \\ldots, \\h_{(1)}$\n",
    "so\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\y_\\tp}{\\partial W_{hh}} = \\frac{\\partial \\y_\\tp}{\\partial \\h_\\tp}\n",
    " \\sum_{k=0}^\\tt {  \\frac{\\partial \\h_\\tp}{\\partial \\h_{(\\tt -k)}} \\frac{\\partial \\h_{(\\tt -k)}}{\\partial \\W_{hh}} }\n",
    "$$\n",
    "\n",
    "The summation: $\\frac{\\partial \\h_\\tp}{\\partial \\W_{hh}}$, through all intermediate $\\h_{(\\tt -k)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The problematic term for us is\n",
    "$$\n",
    "\\frac{\\partial \\h_\\tp}{\\partial \\h_{(\\tt-k)}}\n",
    "$$\n",
    "\n",
    "It can be computed by the Chain Rule as\n",
    "$$\n",
    "\\frac{\\partial \\h_\\tp}{\\partial \\h_{(\\tt-k)}} = \\prod_{u=0}^{\\tt-1} { \\frac{\\partial \\h_{(\\tt -u)}}{\\partial \\h_{(\\tt - u -1) }}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Each term\n",
    "$$\n",
    "\\frac{\\partial \\h_{(\\tt -u)}}{\\partial \\h_{(\\tt - u-1)}}\n",
    "$$\n",
    "results in a term $\\W_{hh}$ so the repeated product compute matrix $\\W_{hh}$ raised to the power $k$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For simplicity,  suppose $\\W_{hh}$ were a scalar\n",
    "- if $\\W_{hh} < 1$ then repeatedly multiply $\\W_{hh}$ by itself approaches $0$\n",
    "- if $\\W_{hh} > 1$ then repeatedly multiply $\\W_{hh}$ by itself approaches $\\infty$\n",
    "\n",
    "In other words:\n",
    "- as the distance between time steps $\\tt$ and $(\\tt -k)$ increases\n",
    "- the gradient (for the weight  update) either vanishes or explodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Since this term is used in the update for our weights\n",
    "- updates  will either be erratic (too big) \n",
    "- or\n",
    "non-existent, hampering learning of weights.\n",
    "\n",
    "This was not necessarily a problem in non-recurrent networks \n",
    "- because each layer had a different\n",
    "weight matrix.\n",
    "\n",
    "What an RNN does that helps it be parsimonius in number of parameters\n",
    "- by sharing the weights across\n",
    "all time steps\n",
    "- hurts us in learning.\n",
    "                                              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For the general case where $\\W_{hh}$ is a matrix\n",
    "- we can show the same resul with the eigenvalues of the matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Controlling exploding gradients by clipping\n",
    "In theory, we can control the explosion by clipping the gradient $\\frac{\\partial \\loss}{\\partial W_i}$.\n",
    "\n",
    "We are still left with the vanishing gradient problem.\n",
    "\n",
    "This means that we can't learn long-term dependencies (i.e., too many steps backward).\n",
    "\n",
    "This will be \"solved\" by introducing recurrent architectures that address this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Long sequences in Tensorflow/Keras\n",
    "\n",
    "Dealing with something as \"simple\" as sequences can be surprisingly difficult in Tensorflow/Keras.\n",
    "- One is required to manually break up long sequences into multiple, shorter subsequences\n",
    "- The ordering of the examples in a mini-batch now becomes relevant\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Consider a long sequence $\\x^\\ip$ of length $n$.\n",
    "\n",
    "The \"natural\" way to represent this $\\X$ is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\n",
    "\\X = \n",
    "\\begin{pmatrix}\n",
    "\\x^{(1)}_{(1)} & \\x^{(1)}_{(2)} & \\ldots & \\x^{(1)}_{(n^{(1)})} \\\\\n",
    "\\x^{(2)}_{(1)} & \\x^{(2)}_{(2)} & \\ldots & \\x^{(2)}_{(n^{(2)})} \\\\\n",
    "\\vdots\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "for equal example lengths $n^{(1)} = n^{(2)} \\ldots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Suppose that the example lengths $n$ is too big.\n",
    "\n",
    "That is, they areto be broken up into subsequences of length $n'$.\n",
    "\n",
    "There will be $n/n'$ such subsequences.\n",
    "\n",
    "We write $\\x^{(i, \\alpha)}$ to denote subsequence number $\\alpha$ in examples $i$.\n",
    "- The elements of this subsequence are $\\x^{(i,\\alpha)}_\\tp$ for $1 \\le \\tt \\le n'$.\n",
    "- So $\\x^{(i,\\alpha +1)}_{(1)}$ follows $\\x^{(i,\\alpha)}_{(n')}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "TensorFlow (as of the time of this writing) has limited primitive concepts\n",
    "- examples *within* batches are unrelated\n",
    "- example $i$ of one batch *can* be made to be related to example $i$ of the following batch\n",
    "    - optional flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To get adjacent subsequences of one sequence to be treated in the proper order by TensorFlow:\n",
    "- Define the number of minibatches to be $n/n'$, which is the number of subsequences\n",
    "- Each subsequence of example $i$ should be at the *same position* within each of the $n/n'$ minibatches\n",
    "- Set RNN optional parameter `stateful=True`\n",
    "- When fitting the model: set `shuffle=False`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\n",
    "\\text{Minibatch 1} = \n",
    "\\begin{pmatrix}\n",
    "\\x^{(1)}_{(1)} & \\x^{(1)}_{(2)} & \\ldots & \\x^{(1)}_{(n')} \\\\\n",
    "\\x^{(2)}_{(1)} & \\x^{(2)}_{(2)} & \\ldots & \\x^{(2)}_{(n')} \\\\\n",
    "\\vdots\n",
    "\\end{pmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\n",
    "\\text{Minibatch 2} = \n",
    "\\begin{pmatrix}\n",
    "\\x^{(1)}_{(n' +1)} & \\x^{(1)}_{(n' +2)} & \\ldots & \\x^{(1)}_{(n' +n')} \\\\\n",
    "\\x^{(2)}_{(n' +1)} & \\x^{(2)}_{(n' +2)} & \\ldots & \\x^{(2)}_{(n' +n')} \\\\\n",
    "\\vdots\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`stateful=True`\n",
    "- TensorFlow *will not** reset the state of the RNN at the beginning of a new minibatch (for each example in the minibatch)\n",
    "\n",
    "This means that \n",
    "- the example at position $i'$ within each minibatch share state ($\\h$)\n",
    "- we've ordered the minibatches in the correct ordering of subsequences\n",
    "- so the result is that the entirety of example $i$'s time steps update the same state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The main difference from the simple method (one example with a long sequence)\n",
    "- we've broken the example up into subsequences\n",
    "- that are related by common position within adjacent minibatches\n",
    "    -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`shuffle=False`\n",
    "- Examples in adjacent minibatches must be in the *same* order, so don't shuffle them !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sequences: Variable length\n",
    "\n",
    "There are lots of small potholes one encounters with sequences.\n",
    "\n",
    "What is the examples of my training set have widely varying lengths ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Within a batch, short examples may behave differently than long examples:\n",
    "    - Maybe learn less in short examples, noisier gradient updates\n",
    "    \n",
    "- Padding sequences to make them equal length\n",
    "    - Pad at the start ? Or at the end ?\n",
    "\n",
    "The general advice is to arrange your data so that an epoch contains examples of similar lengths.\n",
    "- You may require multiple fittings, one per length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Residual connections: a gradient highway\n",
    "[Deep Residual Learning](https://arxiv.org/abs/1512.03385)\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>TL;DR</b> \n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>We have encountered the Vanishing Gradient problem several times.</li>\n",
    "        <li>This is a major impediment to training deep (many layers) networks.</li>\n",
    "        <li>The solution is to give the gradient a path to flow backward undiminished.</li>\n",
    "        <li>This simple solution is called a Skip or Residual Connection</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Consider two layers of a NN\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\y_{(\\ll-1)}      & = & a_{(\\ll-1)}      & \\left( f_{(\\ll-1)}( \\y_{(\\ll-2)} ) \\right) \\\\\n",
    "\\y_{\\llp} & = & a_\\llp & \\left( f_\\llp ( \\y_{(\\ll-1)} ) \\right) \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Suppose we modify layer $\\ll +1$ by adding $\\y_{(\\ll-1)}$ to the  output\n",
    "\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\y_{(\\ll-1)}      & = & a_{(\\ll-1)}      & \\left( f_{(\\ll-1)}( \\y_{(\\ll-2)} ) \\right) \\\\\n",
    "\\y_{\\llp} & = & a_\\llp & \\left( f'_\\llp ( \\y_{(\\ll-1)} ) \\right) + \\y_{(\\ll-1)}   \\\\\n",
    "\\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If the original and modified 2 layer mini-networks compute the same function from $\\y_{(\\ll-1)}$ to $\\y_{(\\ll+1)}$\n",
    "\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "a_\\llp \\left( f_\\llp( \\y_{(\\ll-1)} ) \\right)   & = & a_\\llp  \\left( f'_\\llp ( \\y_{(\\ll-1)}) \\right)  + \\y_{(\\ll-1)} \\\\\n",
    "a_\\llp \\left( f'_\\llp ( \\y_{(\\ll-1)} ) \\right) & = & a_\\llp \\left( f_\\llp( \\y_{(\\ll-1)} ) \\right) - \\y_{(\\ll-1)}  \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "In other words: \n",
    "- we have forced the modified second layer to learn the \"residual\" of the unmodified layer with respect to $\\y_{(\\ll-1)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This seems strange (and pointless) until you consider the Back Propagation process.\n",
    "\n",
    "Recall how the loss gradient\n",
    "$$\\loss'_\\llp = \\frac{\\partial \\loss}{\\partial y_\\llp}$$\n",
    "\n",
    "propagates backwards\n",
    "$$\n",
    "$$\n",
    "\\begin{array}[lll] \\\\\n",
    "\\loss'_{(\\ll-1)} \n",
    "         & = & \\loss'_\\llp \\frac{\\partial \\y_\\llp}{\\partial \\y_{(\\ll-1)}}\n",
    "\\end{array}\n",
    "$$\n",
    "$$\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the unmodified mini-NN  the local derivative\n",
    "$$\n",
    "\\frac{\\partial \\y_\\llp}{\\partial \\y_{(\\ll-1)}} = \\frac{ a_\\llp \\left(f_\\llp(\\ldots) \\right) }{\\partial \\y_{(\\ll-1)}} \n",
    "$$\n",
    "\n",
    "whereas, in the modified mini-NN the local derivative becomes\n",
    "$$\n",
    "\\frac{\\partial \\y_\\llp}{\\partial \\y_{(\\ll-1)}} = \\frac{ a_\\llp \\left( f'_\\llp (\\ldots)\\right) }{\\partial \\y_{(\\ll-1)}} + 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When $\\frac{\\partial \\y_\\llp}{\\partial \\y_{(\\ll-1)}}$ is multiplied by $\\loss'_\\llp$\n",
    "to obtain $\\loss'_{(\\ll-1)}$\n",
    "- the \"upstream\" loss gradient $\\loss'_{(\\ll-1)}$\n",
    "- flows backwards to layer $\\ll -1$ unmodulate *because of the* $+ 1$ term in the modified local derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This simple trick vanquishes the vanishing gradient !\n",
    "\n",
    "It is one of the major reasons that we are able to train extremely deep NN's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There is another important implication:\n",
    "- adding an additional layer cannot result in increased loss\n",
    "\n",
    "This is because there exists a set of weights $\\W_\\llp$ for which \n",
    "$$\n",
    "a_\\llp \\left( f'_\\llp ( \\y_{(\\ll-1)} ) \\right) = 0\n",
    "$$\n",
    "\n",
    "This means the modified second layer computes the identity functon\n",
    "$$\n",
    "\\y_\\llp = \\y_{(\\ll-1)}\n",
    "$$\n",
    "\n",
    "So if adding the second layer had the potential of increasing loss relative to a one layer network\n",
    "- the optimization would learn the identity function instead, an no increase in loss rsults-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Without the skip connection, it is empircally difficult for a NN to learn an identity function as a layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There is an unresolved debate where to place the \"head\" of the skip connection\n",
    "- insider the activation function\n",
    "- outside the activation function\n",
    "\n",
    "We choose the latter to simplify the derivative expression for the loss gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>\"Plain\" Neural Network</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/ResNet_plain.jpg\" width=800\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Neural Network with skip connection</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/ResNet_skip.jpg\" width=800\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preview: skip connections in LSTM's, GRU's\n",
    "\n",
    "The gradient highway also turns out to be useful in RNN's.\n",
    "\n",
    "There are more powerful variants of the RNN called LSTM and GRU which avoid vanishing gradients, partially \n",
    "through the use of skip connections.\n",
    "\n",
    "These variants enhance the power of skip connections by allowing selective skipping via the use\n",
    "of \"gates\".\n",
    "\n",
    "We will see this shortly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Visualization of RNN hidden state\n",
    "\n",
    "Here is a [visualization](http://karpathy.github.io/2015/05/21/rnn-effectiveness/#visualizing-the-predictions-and-the-neuron-firings-in-the-rnn) of single elements within the hidden state, as they consume the input sequence\n",
    "of *single characters*.\n",
    "\n",
    "The color reflects the intensity (value) of the paricular cell (blue=low, red=high)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>State activations after seeing prefix of input</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Unreasonable_effectiveness_1.png\" width=800></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RNN as a generative model\n",
    "\n",
    "\n",
    "Up to now, an RNN's inputs were a prespecificed vector $\\x$.\n",
    "\n",
    "For each example during training, one element of $\\x$ was fed into the RNN per time-step.\n",
    "\n",
    "Similarly for inference time.\n",
    "\n",
    "This behavior is characteristic of a discriminative network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Consider: Suppose there were **no** inputs (or more precisely: a very short sequence $\\x$ of length $t'$, used to \"prime\" the RNN).\n",
    "\n",
    "Instead, let's set the input at time step $t$ to be the output of step $(t-1)$\n",
    "$$\n",
    "\\x_\\tp = \\y_{(t-1)}\n",
    "$$\n",
    "for $t > t'.\n",
    "\n",
    "Then the RNN would be self-perpetuating, never exhausting its inputs, and generating new outputs\n",
    "*conditional* on previous outputs !\n",
    "\n",
    "This would be a generative form of the RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Training by teacher forcing\n",
    "\n",
    "One way to train this RNN is via a supervised task\n",
    "- given sequence $\\x$ up to time $t$: $\\x_{(1, \\ldots t)}$\n",
    "- target is $\\x_{(t+1)}$\n",
    "\n",
    "This is just ordinary supervised training with a specially constructed input derived from a single sequence $\\x$.  \n",
    "\n",
    "Of course, we would do this for a training set with many sequences, as usual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is similar to a classifier where the class we are trying to predict is the class of the next input\n",
    "element.\n",
    "\n",
    "The only \"trick\" is that, at step $t$, the RNN may output the \"wrong\" value $\\hat{\\x} \\ne \\x_{(t+1)}$.\n",
    "If we fed the wrong $\\hat{\\x}$ as the next input to the RNN during training, the RNN would remain\n",
    "permanently off-track and never learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Instead, during *training*, the next input to the RNN \n",
    "- is **forced** to be the correct input (**is the input at step $t$ \n",
    "\n",
    "$$\n",
    "\\x_{(t+1)} \\text{ or is it } \\x_{(t+1)}\n",
    "$$\n",
    "\n",
    "This type of supervised learning is called *teacher forcing*.\n",
    "\n",
    "During *inference* time, we feed back as input whatever the generated output is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Sampling\n",
    "We have described a deterministic generation process.\n",
    "\n",
    "This would be pretty boring, lacking variety\n",
    "- as well as being problematic for generalization\n",
    "- we would be encouraging\n",
    "the RNN to memorize inputs.\n",
    "\n",
    "In producing the single output, what is really happening is\n",
    "- our classifier has one logit per\n",
    "class\n",
    "- we arbitarily decide to pick the largest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But we can properly view the (post-softmax) output\n",
    "- as a probability vector (elements sum to $1$).\n",
    "\n",
    "Instead of choosing the class with maximum probability\n",
    "- we can sample from the probability space\n",
    "defined by this vector\n",
    "- e.g., if the probability for class $c_1$ is twice as great as that for class\n",
    "$c_2$, the probability of sampling $c_1$ would be twice as great.  \n",
    "\n",
    "There is still some chance\n",
    "that $c_2$is sampled, unlike the deterministic case.\n",
    "\n",
    "If we do this, the generator can create output sequences different from any training example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Training, with Teacher Forcing</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_teacher_forcing_training.jpg\" width=800></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Test time: no forcing</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_teacher_forcing_inference.jpg\" width=800\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generating strange things\n",
    "\n",
    "We haven't specified what each element in sequence $\\x$ is.\n",
    "\n",
    "For text, $x_\\tp$ could be either a character or a word, for example.\n",
    "\n",
    "You'll be surprised how successful an RNN can be when it's task is to consume sequences of characters\n",
    "and predict the next character.\n",
    "\n",
    "Although it hasn't been expicitly programmed to generate valid words, punctuation, etc.,\n",
    "it tends to produce realistic text !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Another interesting fact: these \"character RNN's\" also learn semantically meaningful constructs\n",
    "- the need for nested things to match\n",
    "    - multi-level paranthetical phrases, e.g., \"(this is (very important) I think)\"\n",
    "    - opening/closing markup\n",
    "    - indentation/un-indentation of code blocks\n",
    "\n",
    "This suggests that the hidden state may be learning to \"count\" certain concepts.\n",
    "\n",
    "As we will see in a visualization of the hidden state, and in how LSTM's work, this may in fact be true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "RNN's of this type were quite popular and have been used to generate\n",
    "- Fake [Shakespeare](http://karpathy.github.io/2015/05/21/rnn-effectiveness/#shakespeare), or fake politician-speak\n",
    "- Fake code \n",
    "- Fake [math textbooks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/#algebraic-geometry-latex)\n",
    "- [Click bait headline generator](http://clickotron.com/about)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370.594px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

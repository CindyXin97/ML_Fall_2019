{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{(\\ll)}\n",
       "%\n",
       "\\newcommand{\\tp}{\\mathbf{{(t)}}}\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro `_latex_std_` created. To execute, type its name (without quotes).\n",
      "=== Macro contents: ===\n",
      "get_ipython().run_line_magic('run', 'Latex_macros.ipynb')\n",
      " "
     ]
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# My standard magic !  You will see this in almost all my notebooks.\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Reload all modules imported with %aimport\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "MOVIE_DIR=\"./images\"\n",
    "\n",
    "CREATE_MOVIE = False # True if you have ffmpeg installed\n",
    "\n",
    "import training_models_helper\n",
    "%aimport training_models_helper\n",
    "\n",
    "tmh = training_models_helper.TrainingModelsHelper()\n",
    "\n",
    "gdh = training_models_helper.GradientDescentHelper()\n",
    "\n",
    "import mnist_helper\n",
    "%aimport mnist_helper\n",
    "\n",
    "mnh = mnist_helper.MNIST_Helper()\n",
    "\n",
    "import class_helper\n",
    "%aimport class_helper\n",
    "\n",
    "clh= class_helper.Classification_Helper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradient Descent\n",
    "\n",
    "As we have seen before, Machine Learning is an optimization problem:\n",
    "- minimize Cost/Loss\n",
    "- maximize Utility\n",
    "\n",
    "Most of the models we have seen thus far have relatively simple Cost functions that are amenable to\n",
    "closed form solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But the art of Machine Learning is in crafting Cost functions that express the goals of the problem.\n",
    "\n",
    "So we can no longer count on closed form solutions for the optimization.\n",
    "\n",
    "Gradient Descent is a way of finding the parameters (e.g, $\\Theta$) that minimize\n",
    "a given cost Function, ie., solving optimization problems.\n",
    "\n",
    "The advantage of Gradient Descent is its ability to handle complex Cost Functions.\n",
    "\n",
    "Gradient Descent\n",
    "- minimizes convex functions\n",
    "- is a type of search algorithm\n",
    "- is a critical tool for Deep Learning\n",
    "    - most cost functions won't be amenable to closed form solutions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Cost/Loss, Utility/Optimization: review\n",
    "\n",
    "- The prediction $\\hat{\\y}^{(i)}$ for example $\\x^\\ip$ is perfect if it matches the true label $\\y^\\ip$\n",
    "\n",
    "$$ \\hat{\\y}^\\ip = \\y^\\ip$$\n",
    "\n",
    "- The distance between $\\hat{\\y}^{(i)}, \\y^\\ip$  is called the *Loss* (or *Cost*) for example $i$:\n",
    "\n",
    "$$\n",
    "\\loss^\\ip_\\Theta =  L( \\hat{\\y}^\\ip , \\y) \n",
    "$$\n",
    "\n",
    "where $L(a,b)$ is a function that is $0$ when $a = b$ and increasing as $a$ increasingly differs from $b$.\n",
    "\n",
    "Two versions $L$ that we've seen are Mean Squared Error (for Regression) and Cross Entropy Loss (for classification).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Loss for the entire training set is simply the average (across examples) of the Loss for the example\n",
    "\n",
    "$$\n",
    "\\loss_\\Theta  = { 1\\over{m} } \\sum_{i=1}^m \\loss^\\ip_\\Theta\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Whereas Loss describes how \"bad\" our prediction is, we sometimes refer to the converse -- how \"good\" the prediction is.\n",
    "\n",
    "We call the \"goodness\" of the prediction  the *Utility* $U_\\Theta$.\n",
    "\n",
    "So we could state the optimization objective either as\n",
    "\"minimize Cost\" or \"maximize Utility\".\n",
    "\n",
    "By convention, the DL optimization problem is usually framed as one of minimization (of cost or loss) \n",
    "rather than maximization of utility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- The goal of fitting/training is to solve for the $\\Theta$ that minimizes the training set loss \n",
    "$L_\\Theta$ (or conversely, maximizes the Utility $U_\\Theta$.\n",
    "\n",
    "- The method for finding $\\Theta$ is called optimization.\n",
    "\n",
    "- There is one optimization method that we will study in depth: Gradient Descent.\n",
    "\n",
    "- We can use this in Classical ML but it will become a key tool once we move on to Deep Learning.\n",
    "\n",
    "- One focus of this lecture will be variations on Gradient Descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradients\n",
    "\n",
    "Gradient Descent is a method for optimizing the Optimization Objective.\n",
    "\n",
    "It works for any model but we will illustrate it with Linear Regression.\n",
    "\n",
    "For Regression with MSE as the loss, $L(a,b) = (a-b)^2$,so the loss for example $i$ is:\n",
    "$$\n",
    "\\loss^\\ip_\\Theta = (\\text{error}^\\ip)^2\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\text{error}^\\ip = \\hat{\\y}^\\ip - \\y^\\ip\n",
    "$$\n",
    "\n",
    "The average cost\n",
    "\n",
    "$$\n",
    "\\begin{array}{lll}\n",
    "\\loss_\\Theta = \\text{MSE}(\\X, \\Theta) = {1 \\over m} \\sum_{i=1}^m { (\\text{error}^{(i)} )^2}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "where $\\X$ is the entire training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There may be added elements (e.g., regularization constraints) of the objective as well (discussed later). \n",
    "\n",
    "A function of $\\Theta$, $\\loss_\\Theta$, can be minimized by taking its derivative \n",
    "with respect to $\\Theta$ and setting it equal to $0$.\n",
    "\n",
    "This is an equation that can be solved for $\\Theta$.\n",
    "\n",
    "Because $\\Theta$ is a vector, there is one derivative per feature.\n",
    "Hence the vector of derivatives (called the **gradient**) is\n",
    "\n",
    "$\n",
    "\\nabla_\\Theta \\loss_\\Theta =\n",
    "\\begin{pmatrix}\n",
    " \\frac{\\partial}{\\partial \\Theta_0} \\loss_\\Theta \\\\\n",
    " \\frac{\\partial}{\\partial \\Theta_1} \\loss_\\Theta\\\\\n",
    " \\vdots \\\\\n",
    " \\frac{\\partial}{\\partial \\Theta_n} \\loss_\\Theta\n",
    "\\end{pmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For $\\loss_\\Theta = \\text{MSE}(\\X, \\Theta)$\n",
    "$$\n",
    "\\nabla_\\Theta \\loss_\\Theta =\n",
    "\\begin{pmatrix}\n",
    " \\frac{\\partial}{\\partial \\Theta_0} \\text{MSE}(X, \\boldsymbol{\\theta}) \\\\\n",
    " \\frac{\\partial}{\\partial \\Theta_1} \\text{MSE}(X, \\boldsymbol{\\theta}) \\\\\n",
    " \\vdots \\\\\n",
    " \\frac{\\partial}{\\partial \\Theta_n} \\text{MSE}(X, \\boldsymbol{\\theta})\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\n",
    "\\begin{array}{lll}\n",
    "{ { \\partial  }\\over { \\partial{ \\theta_j} } } { \\text{MSE}(X, \\Theta) }& = & {1 \\over m} \\sum_{i=1}^m { \\partial \\over { \\partial \\theta_j } } { ( \\text{error}^{(i)})^2 }\\\\\n",
    "& = & {1 \\over m} \\sum_{i=1}^m { 2 \\times \\text{error}^{(i)} \\times { \\partial \\over { \\theta_j} }} {\\text{error}^{(i)} }\\\\\n",
    "& = & {2 \\over m} \\sum_{i=1}^m { \\text{error}^{(i)} \\times { \\partial \\over { \\theta_j} }} { \\hat{\\y^\\ip } } &\\text{ since } \\text{error}^\\ip = \\hat{\\y}^\\ip - \\y^\\ip, \\y^\\ip \\text{ is constant}\\\\\n",
    "&= &  {2 \\over m} \\sum_{i=1}^m { \\text{error}^{(i)} \\times { \\x^\\ip_j} } & \\text{ since } \\hat{y}^\\ip = \\Theta^T \\cdot \\x = \\sum_{k=1}^n { \\Theta_k \\x^\\ip_k}  \\\\ & & &\\text{ and } \\Theta_j \\x^\\ip_j \\text{ is only term involving } \\Theta_j \\\\\n",
    "\\end{array}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Thus the gradient for Linear Regression can be written in matrix form as\n",
    "\n",
    "$\n",
    "\\nabla_{\\boldsymbol{\\theta}}\\, \\text{MSE}(X, \\boldsymbol{\\theta}) =\n",
    " = \\dfrac{2}{m} \\X^T ( \\theta^T \\X - \\mathbf{y})\n",
    "$\n",
    "\n",
    "This will be particularly useful when working with NumPy as the gradient calculation is a vector operation that is implemented so as to be fast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Batch Gradient Descent\n",
    "\n",
    "The basic algorithm is:\n",
    "1. Initialize $\\Theta$ randomly\n",
    "1. Repeat until done\n",
    "    1. Compute the Gradient\n",
    "    1. Update $\\Theta$ by taking a step in the (negative) direction of the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Intuition**\n",
    "\n",
    "- Our objective is to minimize the Average Loss $\\loss_\\Theta$\n",
    "- The gradient, with respect to $\\Theta_j$, is the increase in $\\loss_\\Theta$ for a unit change in $\\Theta_j$\n",
    "    - we multiply by -1 to *decrease* the loss\n",
    "- So by taking a step (size to be determined) in the negative direction of the gradient we decrease $\\loss_\\Theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's illustrate Batch Gradient Descent on an example.\n",
    "\n",
    "First, we use sklearn's `LinearRegression` as a baseline against which we will compare the $\\Theta$ obtained from\n",
    "Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_lr, y_lr = gdh.gen_lr_data()\n",
    "clf_lr = gdh.fit_lr(X_lr,y_lr)\n",
    "fig, ax = gdh.plot_lr(X_lr, y_lr, clf_lr)\n",
    "\n",
    "theta_lr = (clf_lr.intercept_, clf_lr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now let's perform Batch Gradient Descent and compare the $\\Theta$'s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.99360578e-15],\n",
       "       [-7.99360578e-15]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gd_theta = gdh.batchGradientDescent_lr(X_lr, y_lr)\n",
    "theta_lr - gd_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The $\\Theta$'s are equal up to 15 decimal points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's look at the code for Batch Gradient Descent and examine the details"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "alpha = 0.1\n",
    "n_iterations = 1000\n",
    "m = 100\n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - alpha * gradients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- You can see the code that implements the steps described in English\n",
    "    - `alpha` is the step size: how fast we adjust $\\Theta$ in the direction of the gradient\n",
    "    - `X_b` is the matrix\n",
    "        - whose first column is $1$\n",
    "        - whose other columns are the non-intercept features\n",
    "    - `X_b.dot(theta)` are the predicted values for all observations\n",
    "    - `X_b.dot(theta) - y` are the errors for all observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Intuition**\n",
    "\n",
    "- Why is the proportional to `gradients`?\n",
    "    - When the gradient is large, we are far from the minimum $\\loss_\\Theta$: take a big step\n",
    "    - When the gradient is small, we are close to the minimum $\\loss_\\Theta$: take a small step\n",
    "- Why $\\alpha < 1$ ?\n",
    "    - If we take too big a step, we may overshoot the minimum, as we will see"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Since the $\\Theta$'s computed by Gradient Descent and Linear Regression are the same, it's no surprise that the predictions are too.\n",
    "- as demonstrated in the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True],\n",
       "       [ True]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = np.array([[0], [2]])\n",
    "gd_y_pred = gdh.predict(X_new, theta_lr)\n",
    "clf_y_pred = clf_lr.predict(X_new)\n",
    "\n",
    "gd_y_pred == clf_y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Batch gradient descent: the movie\n",
    "\n",
    "We can hopefully gain intuition by watching Gradient Descent at work.\n",
    "- you will see the effect of each update of $\\Theta$\n",
    "- you will see the effect of changing $\\eta$, which scales the step size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "movie_file = os.path.join(MOVIE_DIR,'batch_gradient_descent_eta_10.mp4')\n",
    "\n",
    "if CREATE_MOVIE:\n",
    "    gd_anim = gdh.create_movie(X_lr, y_lr, n_iterations=10)\n",
    "    gd_anim.save(movie_file, codec='h264')\n",
    "\n",
    "\n",
    "if CREATE_MOVIE:\n",
    "    gdh.show_movie(gd_anim)\n",
    "else:\n",
    "    print(\"To view movie:\\n Use link in following cell, or use browser to visit file {f}\".format(f=movie_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[Movie](images/batch_gradient_descent_eta_10.mp4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Initializing $\\Theta$\n",
    "What would have happened if, instead of initializing $\\Theta$ to random numbers \n",
    "- we had initialized it to $0$ ?\n",
    "- we had initialized it to a very large number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Step size\n",
    "\n",
    "What's a good choice for `alpha` ?  We had used 0.1 and obtained convergence in around 10 steps.\n",
    "\n",
    "Le'ts try a smaller step size: `alpha` = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "movie_file = os.path.join(MOVIE_DIR,'batch_gradient_descent_eta_02.mp4')\n",
    "\n",
    "if CREATE_MOVIE:\n",
    "    gd_anim_eta_02 = gdh.create_movie(X_lr, y_lr, alpha=0.02, n_iterations=30)\n",
    "    gd_anim_eta_02.save(movie_file, codec='h264')\n",
    "\n",
    "\n",
    "if CREATE_MOVIE:\n",
    "    gdh.show_movie(gd_anim_eta_02)\n",
    "else:\n",
    "    print(\"To view movie:\\n Use link in following cell, or use browser to visit file {f}\".format(f=movie_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[Movie](images/batch_gradient_descent_eta_02.mp4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Like watching paint dry !\n",
    "\n",
    "How about something bigger ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "movie_file = os.path.join(MOVIE_DIR,'batch_gradient_descent_eta_45.mp4')\n",
    "if CREATE_MOVIE:\n",
    "    gd_anim_eta_45 = gdh.create_movie(X_lr, y_lr, alpha=0.45, n_iterations=20)\n",
    "    gd_anim_eta_45.save(movie_file, codec='h264')\n",
    "\n",
    "\n",
    "if CREATE_MOVIE:\n",
    "    gdh.show_movie(gd_anim_eta_45)\n",
    "else:\n",
    "    print(\"To view movie:\\n Use link in following cell, or use browser to visit file {f}\".format(f=movie_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[Movie](images/batch_gradient_descent_eta_45.mp4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And even bigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "movie_file = os.path.join(MOVIE_DIR,'batch_gradient_descent_eta_50.mp4')\n",
    "\n",
    "if CREATE_MOVIE:\n",
    "    gd_anim_eta_50 = gdh.create_movie(X_lr, y_lr, alpha=0.50, n_iterations=20)\n",
    "    gd_anim_eta_50.save(movie_file, codec='h264')\n",
    "\n",
    "\n",
    "if CREATE_MOVIE:\n",
    "    gdh.show_movie(gd_anim_eta_50)\n",
    "else:\n",
    "    print(\"To view movie:\\n Use link in following cell, or use browser to visit file {f}\".format(f=movie_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[Movie](images/batch_gradient_descent_eta_50.mp4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Lost in space !\n",
    "- the vertical intercept magnitude increases until it is off the screen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning rate schedule\n",
    "\n",
    "We see from the above that if the step size is too small, it takes long to converge.\n",
    "\n",
    "But if the step size is too big, we may overshoot.\n",
    "\n",
    "An adaptive learning rate schedule may be the solution:\n",
    "- take big steps at first\n",
    "- take smaller steps toward end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "t0, t1 = 5, 50  # learning schedule hyperparameters\n",
    "\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "t = np.linspace(0, 10, 10)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax  = fig.add_subplot(1,1,1)\n",
    "_ =ax.plot(t, learning_schedule(t))\n",
    "_ = ax.set_xlabel(\"time\")\n",
    "_ = ax.set_ylabel(\"eta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### When to stop\n",
    "\n",
    "Can we do better than running for a fixed number of iterations ?\n",
    "Yes:\n",
    "- Let $\\text{Cost}_{t}$ be the Cpst Function at step $t$\n",
    "- Stop if\n",
    "    - $\\text{Cost}_{t-1} - \\text{Cost}_{t} < \\epsilon $\n",
    "    - That is: stop if improvement of Cost Function is not big enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A word on derivatives\n",
    "\n",
    "Preview of part 2 of the course:\n",
    "- the derivatives we used were *analytic* and not numerical approximations\n",
    "- how can we automate calculation of analytic derivatives ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other cost functions\n",
    "\n",
    "- Ridge Regression Cost Function\n",
    "    - MSE, with a penalty large $\\Theta$\n",
    "        - it's easy to compute the derivative of this cost function\n",
    "        - try Minibatch Gradient Descent on this Cost Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Gradient Boosing\n",
    "\n",
    "In the Decision Tree lecture we described Gradient Boosting (using Decision Tree Regression as an example).\n",
    "\n",
    "Herei\n",
    "- We built a *sequence* of Trees $T_{(0)}, T_{(1)}, \\ldots$\n",
    "- The relevance to us is that this resulted in a sequence of predictions $\\hat{\\y}_{(0)}, \\y_{(1)}, \\ldots$\n",
    "- We updated $\\hat{\\y}$ via\n",
    " $$\\hat{\\y}_\\tp = \\hat{\\y}_{(t-1)} + \\alpha *\\hat{\\e}_\\tp$$\n",
    "     - where $\\e_\\tp = \\y - \\hat{\\y}_{(t-1)}$\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For an MSE loss function\n",
    "$$\n",
    "\\loss^\\ip = ( \\hat{\\y}_{(t-1)} - \\y )^2\n",
    "$$\n",
    "\n",
    "the derivative with respect to $\\hat{\\y}_{(t-1)}$ is\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\frac{\\partial \\loss^\\ip }{\\partial \\hat{\\y}_{(t-1)}}  & = &  2 * (  \\hat{\\y}_{(t-1)} - \\y) & \\text{ by chain rule}\\\\\n",
    "& = & -2 * \\e_\\tp \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So the update rule for Gradient Boosting is the same as for Gradient Descent !\n",
    "\n",
    "Thus Gradient Boosting, which was originally described informally \n",
    "- is the minimization an MSE loss function by Gradient Descent.\n",
    " - n.b., we often describe a loss function as 0.5 time that described above\n",
    "     - in to cancel out the $2$ from the derivative of the sqaured error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Improvements to Gradient Descent\n",
    "\n",
    "[Simon Ruder survey](https://arxiv.org/abs/1609.04747)\n",
    "\n",
    "[Gradient Descent Cheatsheet](https://towardsdatascience.com/10-gradient-descent-optimisation-algorithms-86989510b5e9)\n",
    "\n",
    "The update step\n",
    "$$\n",
    "\\Theta = \\Theta - \\alpha * \\frac{\\partial \\loss_\\Theta }{\\partial \\Theta}\n",
    "$$\n",
    "\n",
    "\n",
    "where $\\alpha$ is the learning rate.\n",
    "\n",
    "The improvements to Gradient Descent modify\n",
    "- $\\alpha$, the learning rate\n",
    "- $\\frac{\\partial \\loss_\\Theta }{\\partial \\Theta}$ the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In order to be able to flexibly change the definiton of both the gradient and the learning rate\n",
    "at each time step $t$,\n",
    "e will re-write the update step at time $t$ as\n",
    "\n",
    "$$\n",
    "\\Theta_\\tp = \\Theta_{(t-1)} -  \\alpha' * V_\\tp\n",
    "$$\n",
    "\n",
    "$V_\\tp$ will be our modified gradient and $\\alpha'$ our modified learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Momentum: modify the gradient\n",
    "\n",
    "In vanilla Gradient Descent, the gradients at time $t-1$ and time $t$ are completely independent.\n",
    "\n",
    "This has the potential for gradients to rapidly change direction (recall, they are a vector).\n",
    "\n",
    "To smooth out jumps we could compute a modified gradient $V_\\tp$ as:\n",
    "$$\n",
    "V_\\tp = \\beta_V * V_{(t-1)} + (1 - \\beta_V) * \\frac{\\partial \\loss_\\Theta }{\\partial \\Theta}\n",
    "$$\n",
    "\n",
    "(Initialize $V_0 = 0$)\n",
    "\n",
    "That is, the modified gradient is a weighted combination of the previous gradient and the new gradient.\n",
    "\n",
    "Typically $\\beta_V \\approx 0.9$ so the old gradient dominates.\n",
    "\n",
    "$V_\\tp$ is the exponentially weighted moving average of the gradient.\n",
    "\n",
    "Hence, there is \"momentum\" in the gradients in that they can't jump suddenly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RMSprop: Modify the learning rate\n",
    "\n",
    "Let\n",
    "$$\n",
    "S_\\tp = \\beta_S * S_{(t-1)} + (1 -\\beta_S) * \\left( \\frac{\\partial \\loss_\\Theta }{\\partial \\Theta} \\right)^2\n",
    "$$\n",
    "\n",
    "That is, $S_\\tp$ is the exponentially weighted *variance* of the gradient.\n",
    "\n",
    "(Initialize $S_0 = 0$)\n",
    "\n",
    "Rather than using a learning rate of $\\alpha$, the RMSprop algorithm uses\n",
    "$$\n",
    "\\alpha' = \\frac{1}{\\sqrt{S_\\tp + \\epsilon}} * \\alpha\n",
    "$$\n",
    "\n",
    "The intuition is that if the gradient with respect to $\\Theta_j$ is noisy (i.e., large variance)\n",
    "we want to damp updates in that component.\n",
    "\n",
    "This also has the advantage that a rarely updated element $\\Theta_i$, having a low variance,\n",
    "will have a relatively larger update when it is encountered than a more frequently encountered feature.\n",
    "\n",
    "Typically $\\beta_S \\approx 0.9$ so the old variance dominates.\n",
    "\n",
    "Why the extra $\\epsilon$ ?  We've seen this before (e.g., $\\log(x + \\epsilon)$):\n",
    "it's to avoid mathematical issues of certain functions (inverse, log) when the argument is $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## AdaM: Modify both the gradient and the learning rate\n",
    "\n",
    "The AdaAM (Adaptive Moment) algorithm modifies both the gradient and learning rates via\n",
    "exponentially moving averages of the gradient as well as its variance.\n",
    "\n",
    "$$\n",
    "\\begin{array}[lll] \\\\\n",
    "V_\\tp = \\beta_V * V_{(t-1)} + (1 - \\beta_V) \\frac{\\partial \\loss_\\Theta }{\\partial \\Theta} \\\\\n",
    "S_\\tp = \\beta_S * S_{(t-1)} + (1 -\\beta_S) * \\left( \\frac{\\partial \\loss }{\\partial \\Theta} \\right)^2 \\\\\n",
    "\\alpha' = \\frac{1}{\\sqrt{S_\\tp + \\epsilon}} * \\alpha\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bias correction\n",
    "\n",
    "You will have observed that we initialized to $0$ the moving averages for gradients ($V_0 = 0$) and\n",
    "the variance of the gradients ($S_0 = 0$).\n",
    "\n",
    "So the values are \"biased\" towards 0 with the bias having greatest effect for small $t$ (i.e., when the number\n",
    "of \"actual\" values is small).\n",
    "\n",
    "We can correct for the bias by dividing by $(1 - \\beta^t)$:\n",
    "\n",
    "$$\n",
    "\\begin{array}[lll] \\\\\n",
    "\\hat{V} & = & \\frac{V_\\tp}{1 - \\beta_V^t} \\\\\n",
    "\\hat{S} & = & \\frac{S_\\tp}{1 - \\beta_S^t} \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
